{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"91701e4f875d40858ca829afa2c280bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0eb98b1f46ae48c88cca8692d785f339","IPY_MODEL_9d605910a22a47bfae2fa8e860dcb0de","IPY_MODEL_83bf3bff7ea448e2b485f4595afd14f1"],"layout":"IPY_MODEL_be01158178a948c992000b5bceac1fed"}},"0eb98b1f46ae48c88cca8692d785f339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef30293fc0c3444889689110778c6b48","placeholder":"​","style":"IPY_MODEL_4789696aafad42308cd1c700ffa9bb81","value":"tokenizer_config.json: 100%"}},"9d605910a22a47bfae2fa8e860dcb0de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_645c2794667e4d7cae385311b9038392","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d267ed3991ce40e8bb1bc44f189ee0bf","value":48}},"83bf3bff7ea448e2b485f4595afd14f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2aeffa5ed00436ab7d3c817f1cbad11","placeholder":"​","style":"IPY_MODEL_fddce167fac34d0d80bd547606c251b7","value":" 48.0/48.0 [00:00&lt;00:00, 906B/s]"}},"be01158178a948c992000b5bceac1fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef30293fc0c3444889689110778c6b48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4789696aafad42308cd1c700ffa9bb81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"645c2794667e4d7cae385311b9038392":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d267ed3991ce40e8bb1bc44f189ee0bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2aeffa5ed00436ab7d3c817f1cbad11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fddce167fac34d0d80bd547606c251b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da8e730353ef4f2baeb600fc3827b010":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5e5744121944dc6823deaaad7869046","IPY_MODEL_c6f780391ef5415b865bc23e34c352bc","IPY_MODEL_64881841452049c39af90b9f8b3f4909"],"layout":"IPY_MODEL_700d96bb105a44b891653cef81fe063a"}},"d5e5744121944dc6823deaaad7869046":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf5531c2335f47b7ba9e21d1921bfd0c","placeholder":"​","style":"IPY_MODEL_90494a5fca3a46f9b2ff65a5da46f51b","value":"config.json: 100%"}},"c6f780391ef5415b865bc23e34c352bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3fa9dc8d9b246c3b775cdd9e50f8b36","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8985d2c533ed486c9a64cbe0a5afb0e8","value":570}},"64881841452049c39af90b9f8b3f4909":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a297934c0bcb4f3193bc81d1eeaa1f94","placeholder":"​","style":"IPY_MODEL_0e5533d6d05d4f6287f1a2446b79384d","value":" 570/570 [00:00&lt;00:00, 19.2kB/s]"}},"700d96bb105a44b891653cef81fe063a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf5531c2335f47b7ba9e21d1921bfd0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90494a5fca3a46f9b2ff65a5da46f51b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3fa9dc8d9b246c3b775cdd9e50f8b36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8985d2c533ed486c9a64cbe0a5afb0e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a297934c0bcb4f3193bc81d1eeaa1f94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e5533d6d05d4f6287f1a2446b79384d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adc76903a500407ba5805fbb9a722631":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e306f99b0c142af9f12ada4c41f4d4f","IPY_MODEL_f9ed4614e164498d8c453546b576c594","IPY_MODEL_52129229474340d09677c845fbeccd35"],"layout":"IPY_MODEL_f3e609de7937437597b2bb5e2988cbc8"}},"8e306f99b0c142af9f12ada4c41f4d4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_759826601ea5456b8f8d8d397f1b7315","placeholder":"​","style":"IPY_MODEL_f65414e703444ea88029996647608b1b","value":"vocab.txt: 100%"}},"f9ed4614e164498d8c453546b576c594":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba61bc4a0e8a44a2b6df212201de43b6","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b83852e33cdc4aafbe97698dcc5f7ffa","value":231508}},"52129229474340d09677c845fbeccd35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa8d647634741348017605c8c34b581","placeholder":"​","style":"IPY_MODEL_f296cd551c9a41e4aca633b922200a2a","value":" 232k/232k [00:00&lt;00:00, 552kB/s]"}},"f3e609de7937437597b2bb5e2988cbc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"759826601ea5456b8f8d8d397f1b7315":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f65414e703444ea88029996647608b1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba61bc4a0e8a44a2b6df212201de43b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b83852e33cdc4aafbe97698dcc5f7ffa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fa8d647634741348017605c8c34b581":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f296cd551c9a41e4aca633b922200a2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b82f297d53f8443aa206396d2c488254":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adbf3d646df8422393fed4a78b886e60","IPY_MODEL_2e12023f40c7432eaf3f8abeff8ef8b8","IPY_MODEL_43882434c7fd45a1bbcc5fe9968be22d"],"layout":"IPY_MODEL_6c62e4bcd2514e88a2232de084ac0b88"}},"adbf3d646df8422393fed4a78b886e60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6702f3258ff24ee28a8fba96af10316d","placeholder":"​","style":"IPY_MODEL_aa882d82c243454388ea68edbe7a7284","value":"tokenizer.json: 100%"}},"2e12023f40c7432eaf3f8abeff8ef8b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19b50a6abdee41178f34f8f56a6053c4","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44e2c1aa82eb4963b02dadf7b5dba6b0","value":466062}},"43882434c7fd45a1bbcc5fe9968be22d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66ca706c154c4010afbaae5885572494","placeholder":"​","style":"IPY_MODEL_41d188a6db824a4e8c8d3449170ab504","value":" 466k/466k [00:00&lt;00:00, 1.10MB/s]"}},"6c62e4bcd2514e88a2232de084ac0b88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6702f3258ff24ee28a8fba96af10316d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa882d82c243454388ea68edbe7a7284":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19b50a6abdee41178f34f8f56a6053c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44e2c1aa82eb4963b02dadf7b5dba6b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66ca706c154c4010afbaae5885572494":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41d188a6db824a4e8c8d3449170ab504":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f9ff2719215464abe8f769656975815":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7bbbf2a035b4faa98e387b55ca0bfc0","IPY_MODEL_dbe653d11b0640fb8d9925cff30dc1a6","IPY_MODEL_5af10585b9c44552945e43b236ba28c9"],"layout":"IPY_MODEL_e8e1d4cfad2f4ab2ac9acdfc97a37414"}},"b7bbbf2a035b4faa98e387b55ca0bfc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcd854374c354c9b8f05447b564eb6cc","placeholder":"​","style":"IPY_MODEL_a8bdc12eba814807b27a75f9265af2be","value":"model.safetensors: 100%"}},"dbe653d11b0640fb8d9925cff30dc1a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14b2e806d1534e8fafc6a1879bc716aa","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2aeda233a7314cdbb16460d3303baea5","value":440449768}},"5af10585b9c44552945e43b236ba28c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7010732eb164fd98a0699bd11a042e1","placeholder":"​","style":"IPY_MODEL_9e74ed551c3a437c9a9a0ce616ecb559","value":" 440M/440M [00:05&lt;00:00, 113MB/s]"}},"e8e1d4cfad2f4ab2ac9acdfc97a37414":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcd854374c354c9b8f05447b564eb6cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8bdc12eba814807b27a75f9265af2be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14b2e806d1534e8fafc6a1879bc716aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aeda233a7314cdbb16460d3303baea5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a7010732eb164fd98a0699bd11a042e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e74ed551c3a437c9a9a0ce616ecb559":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["We will build the real news vs fake news detection engine. We want to demonstrate how this pipeline can be adapted to your organization's specific needs. Instead of using a pre-built dataset, we will download a dataset from Kaggle and utilize it in our fine-tuning process. This approach will help illustrate how the pipeline can be tailored to work with custom datasets in real-world applications.\n","Here's an outline of the fine-tuning process\n","1. Import required libraries and packages\n","\n","2. Load the dataset. Download the data from kaggle and save it on your drive.\n","3. Load pre-trained BERT tokenizer:\n","\n","\n","4. Prepare the dataset:\n","\n","\n","  * Tokenize the text using the BERT tokenizer\n","  * Create attention masks\n"," * Split the dataset into training and validation sets\n","  * Create a custom PyTorch dataset class (TextClassificationDataset)\n","  * Instantiate the custom dataset for both training and validation sets\n","  * Create PyTorch DataLoader\n","  \n","4. Load a pre-trained BERT model for sequence classification using the Hugging Face Transformers library\n","5. Setup Accelarator environment\n","6. Fine-tune the model:\n","\n","7. Evaluate the model:\n","  *Calculate  metrics, such as F1 score, recall, and precision\n","8. Inference:\n","\n","  * Create a function to perform inference on new text input\n"," * Tokenize the input text and convert it to the required format\n"," * Perform inference using the fine-tuned model\n"," * Interpret the model's output and return the predicted class"],"metadata":{"id":"y-o9q5TSzR8a"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install torch\n","!pip install torchtext\n","!pip install accelerate\n","!pip install sentencepiece\n","!pip3 install sacremoses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Od1VluayQBFL","executionInfo":{"status":"ok","timestamp":1730007324957,"user_tz":-540,"elapsed":46916,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}},"outputId":"d67a82a9-6d73-4056-ea1b-89f2beb3971f","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Collecting datasets\n","  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Collecting torchtext\n","  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n","Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchtext\n","Successfully installed torchtext-0.18.0\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n","Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sacremoses\n","Successfully installed sacremoses-0.1.1\n"]}]},{"cell_type":"markdown","source":["# 1. Import required libraries and packages\n"],"metadata":{"id":"WrC8FHao2aon"}},{"cell_type":"code","source":["import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from accelerate import Accelerator\n","\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","from tqdm import tqdm\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AdamW\n","from transformers import get_scheduler"],"metadata":{"id":"rOit_UrJdw9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** MPS=> Apple's Metal Performance Shaders (MPS) is a framework that provides highly optimized, low-level GPU-accelerated functions for deep learning, image processing, and other compute-intensive tasks."],"metadata":{"id":"h2Z31EI_y08Q"}},{"cell_type":"code","source":["def get_device():\n","  device=\"cpu\"\n","  if torch.cuda.is_available():\n","    device=\"cuda\"\n","  elif  torch.backends.mps.is_available():\n","    device='mps'\n","  else:\n","    device=\"cpu\"\n","  return device\n","\n","\n","device = get_device()\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdAE-Xc4cg9D","outputId":"e09c9a30-0309-4543-fbbc-4d4d06f717ec","executionInfo":{"status":"ok","timestamp":1730008275103,"user_tz":-540,"elapsed":619,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["# 2. Load Data\n","1. Reading data from two CSV files: True.csv (real news) and Fake.csv (fake news)\n","2. Cleaning and preprocessing the data in each CSV file\n","3. Concatenating both dataframes into a single dataframe\n","4. The resulting dataframe contains two columns: 'text' for the news content and 'label' for its corresponding category (real or fake)"],"metadata":{"id":"aOT9-e3R15hG"}},{"cell_type":"code","source":["# 구글 드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqRtnUs5XF0i","executionInfo":{"status":"ok","timestamp":1730007410979,"user_tz":-540,"elapsed":28479,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}},"outputId":"8f747779-211d-41b7-c951-2e0ece00fb51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["real=pd.read_csv('/content/drive/MyDrive/Book6/Ch4/True.csv')\n","fake=pd.read_csv('/content/drive/MyDrive/Book6/Ch4/Fake.csv')\n"],"metadata":{"id":"f2YociOdJ4CD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real = real.drop(['title','subject','date'], axis=1)\n","real['label']=1.0\n","fake = fake.drop(['title','subject','date'], axis=1)\n","fake['label']=0.0\n","dataframe=pd.concat([real, fake], axis=0, ignore_index=True)\n"],"metadata":{"id":"IERhOScoLJM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dataframe.sample(frac=0.1).reset_index(drop=True)\n","print(df.head(20))\n","print(len(df[df['label']==1.0]))\n","print(len(df[df['label']==0.0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBU1b0_syl00","outputId":"40a5f232-8594-4303-ea91-b41123938656","executionInfo":{"status":"ok","timestamp":1730007487261,"user_tz":-540,"elapsed":547,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                 text  label\n","0   (Reuters) - Texas Governor Greg Abbott made go...    1.0\n","1   ABOARD AIR FORCE ONE (Reuters) - U.S. Attorney...    1.0\n","2   WASHINGTON (Reuters) - Democratic presidential...    1.0\n","3   Wikileaks released another email showing how p...    0.0\n","4   OSLO (Reuters) - The United States could influ...    1.0\n","5   Hillary Clinton has turned down repeated reque...    0.0\n","6   21st Century Wire says One of the great myths ...    0.0\n","7    Effective as of December 31, 2017, the Patien...    0.0\n","8   McCain: Our intelligence agencies concluded un...    0.0\n","9   SAMARKAND, Uzbekistan (Reuters) - Senior offic...    1.0\n","10  WASHINGTON (Reuters) - A federal court in the ...    1.0\n","11  PARIS (Reuters) - U.S. Secretary of State Rex ...    1.0\n","12  BRUSSELS (Reuters) - EU envoys discussed on We...    1.0\n","13  WASHINGTON (Reuters) - U.S. Senate negotiators...    1.0\n","14  KIEV (Reuters) - The Ukrainian wife of a Chech...    1.0\n","15  NEW YORK (Reuters) - U.S. President-elect Dona...    1.0\n","16  WASHINGTON (Reuters) - The top Republican in t...    1.0\n","17  While the Republican Party rants on and on abo...    0.0\n","18  WASHINGTON/BEIJING (Reuters) - U.S. President ...    1.0\n","19  SACRAMENTO, Calif. (Reuters) - California will...    1.0\n","2116\n","2374\n"]}]},{"cell_type":"markdown","source":["#3.  Load Tokenizer:\n","1. We are using the `bert-base-uncased` tokenizer. We also need to use the corresponding model"],"metadata":{"id":"UEE-QEUW3YyM"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"],"metadata":{"id":"AFP8kHXrzfSL","colab":{"base_uri":"https://localhost:8080/","height":307,"referenced_widgets":["91701e4f875d40858ca829afa2c280bc","0eb98b1f46ae48c88cca8692d785f339","9d605910a22a47bfae2fa8e860dcb0de","83bf3bff7ea448e2b485f4595afd14f1","be01158178a948c992000b5bceac1fed","ef30293fc0c3444889689110778c6b48","4789696aafad42308cd1c700ffa9bb81","645c2794667e4d7cae385311b9038392","d267ed3991ce40e8bb1bc44f189ee0bf","e2aeffa5ed00436ab7d3c817f1cbad11","fddce167fac34d0d80bd547606c251b7","da8e730353ef4f2baeb600fc3827b010","d5e5744121944dc6823deaaad7869046","c6f780391ef5415b865bc23e34c352bc","64881841452049c39af90b9f8b3f4909","700d96bb105a44b891653cef81fe063a","cf5531c2335f47b7ba9e21d1921bfd0c","90494a5fca3a46f9b2ff65a5da46f51b","a3fa9dc8d9b246c3b775cdd9e50f8b36","8985d2c533ed486c9a64cbe0a5afb0e8","a297934c0bcb4f3193bc81d1eeaa1f94","0e5533d6d05d4f6287f1a2446b79384d","adc76903a500407ba5805fbb9a722631","8e306f99b0c142af9f12ada4c41f4d4f","f9ed4614e164498d8c453546b576c594","52129229474340d09677c845fbeccd35","f3e609de7937437597b2bb5e2988cbc8","759826601ea5456b8f8d8d397f1b7315","f65414e703444ea88029996647608b1b","ba61bc4a0e8a44a2b6df212201de43b6","b83852e33cdc4aafbe97698dcc5f7ffa","4fa8d647634741348017605c8c34b581","f296cd551c9a41e4aca633b922200a2a","b82f297d53f8443aa206396d2c488254","adbf3d646df8422393fed4a78b886e60","2e12023f40c7432eaf3f8abeff8ef8b8","43882434c7fd45a1bbcc5fe9968be22d","6c62e4bcd2514e88a2232de084ac0b88","6702f3258ff24ee28a8fba96af10316d","aa882d82c243454388ea68edbe7a7284","19b50a6abdee41178f34f8f56a6053c4","44e2c1aa82eb4963b02dadf7b5dba6b0","66ca706c154c4010afbaae5885572494","41d188a6db824a4e8c8d3449170ab504"]},"executionInfo":{"status":"ok","timestamp":1730007587035,"user_tz":-540,"elapsed":6084,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}},"outputId":"c1018372-9c25-4c32-8f88-a01c74be05c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91701e4f875d40858ca829afa2c280bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da8e730353ef4f2baeb600fc3827b010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc76903a500407ba5805fbb9a722631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82f297d53f8443aa206396d2c488254"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# 4. Prepare Data\n","The data preparation process for BERT-based uncased models involves tokenizing the text, mapping tokens to `input_ids`, creating attention masks `attention_mask`, , and preparing the labels tensor `labels`. Each element of Dataset Class should be dictionary of following structure.\n","\n","```\n","{'input_ids': torch.Tensor(),'attention_mask':torch.Tensor(), 'labels': torch.Tensor()  }\n","```\n","1. Tokenization: The text input should be tokenized into subwords using BERT's WordPiece tokenizer. This tokenizer converts the text into a format that BERT can understand.\n","\n","2. `input_ids`: Each token from the tokenized text needs to be mapped to an ID using BERT's vocabulary. The resulting input IDs should be in the form of a tensor or array, usually of shape (batch_size, max_sequence_length).\n","3. `attention_mask`: The attention mask is used to differentiate between the actual tokens and padding tokens. It has the same shape as the input IDs tensor, i.e., (batch_size, max_sequence_length). The mask has 1s for actual tokens and 0s for padding tokens.\n","4. `labels`: The labels tensor contains the true class or value for each example in the dataset. It usually has a shape of (batch_size,). For classification tasks, these labels are one-hot-encoded labels"],"metadata":{"id":"4SQ3sVVn3tSi"}},{"cell_type":"code","source":["# (text, label) 형태의 튜플로 구성된 리스트 생성\n","data=list(zip(df['text'].tolist(), df['label'].tolist()))\n","\n","# 다음 함수는 파라미터로 texts와 lables로 구성된 리스트를 가지며\n","# 출력으로 input_ids, attention_mask, labels_out을 생성\n","def tokenize_and_encode(texts, labels):\n","    input_ids, attention_masks, labels_out = [], [], []\n","    for text, label in zip(texts, labels):\n","        encoded = tokenizer.encode_plus(\n","            text, max_length=512, padding='max_length', truncation=True)\n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","        labels_out.append(label)\n","    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels_out)\n","\n","# 튜플을 분리하여 containing texts, containing labels 리스트 생성\n","texts, labels = zip(*data)\n","\n","# 학습 및 검증 데이터셋 분리\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n","\n","# 토큰화\n","train_input_ids, train_attention_masks, train_labels = tokenize_and_encode(train_texts, train_labels)\n","val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(val_texts, val_labels)\n","\n","\n"],"metadata":{"id":"jbJiQDfw9tUI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**It's always good to review the data**\n","1. input_ids\n","  * `0` token value means padded token\n","2. attention_mask\n","  * `1`: corresponding token is real token\n","  * `0`: corresponding token is padded token"],"metadata":{"id":"A0sp2xdP7UPG"}},{"cell_type":"code","source":["print('train_input_ids ',train_input_ids[0].shape ,train_input_ids[0], '\\n'\n","      'train_attention_masks ', train_attention_masks[0] ,train_attention_masks[0], '\\n'\n","      'train_labels', train_labels[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9VcMmQEeQ9H","outputId":"52b74ef6-d554-4b97-bbac-22bbab98710f","executionInfo":{"status":"ok","timestamp":1730007684685,"user_tz":-540,"elapsed":822,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_input_ids  torch.Size([512]) tensor([  101, 14749,  9587, 14478,  2218,  1037,  2811,  3034,  2651,  1999,\n","         2029,  2016,  2056,  2610,  3738,  2018,  2053, 15596,  3426,  2000,\n","         6545, 15528,  3897,  1012,  2008,  1055,  1037,  4682, 15528,  3897,\n","         2018,  2019,  3161, 10943,  2041,  2005,  2010,  6545,  2061,  6222,\n","         2610,  2018,  2296,  3114,  2000,  3288,  2032,  1999,  1012,   102,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0]) \n","train_attention_masks  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0]) \n","train_labels tensor(0.)\n"]}]},{"cell_type":"markdown","source":["### TextClassificationDataset\n","1. For tunning `bert-based-uncased`: each item of Dataset must be of type dictionary with at following  keys:\n","  * input_ids\n","  * attention_mask\n","  * labels\n","2. Thus,  `__getitem__`  should return dictionary of following structure:\n","```\n","{\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_masks[idx],\n","            'labels': self.one_hot_labels[idx]\n","        }\n","```\n","3. one_hot_encode method: A static method that takes in targets (labels) and num_classes as arguments. It converts the given targets into one-hot encoded tensors. The method first converts the targets to long tensors and then initializes a zero tensor of shape (number of samples, num_classes). The scatter_ function is used to place 1.0 in the appropriate position for each sample's label, resulting in a one-hot encoded tensor."],"metadata":{"id":"ZTyeVXy7t-T1"}},{"cell_type":"code","source":["class TextClassificationDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_ids, attention_masks, labels, num_classes=2):\n","        self.input_ids = input_ids\n","        self.attention_masks = attention_masks\n","        self.labels = labels\n","        self.num_classes = num_classes\n","        self.one_hot_labels = self.one_hot_encode(labels, num_classes)\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_masks[idx],\n","            'labels': self.one_hot_labels[idx]\n","        }\n","\n","\n","    @staticmethod\n","    def one_hot_encode(targets, num_classes):\n","        targets = targets.long()\n","        one_hot_targets = torch.zeros(targets.size(0), num_classes)\n","        one_hot_targets.scatter_(1, targets.unsqueeze(1), 1.0)\n","        return one_hot_targets\n","\n","\n","train_dataset = TextClassificationDataset(train_input_ids, train_attention_masks, train_labels)\n","val_dataset = TextClassificationDataset(val_input_ids, val_attention_masks, val_labels)\n"],"metadata":{"id":"7yfXP_F2zVcs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataLoader\n","*italicized text*"],"metadata":{"id":"KQxJ0TU8vGT1"}},{"cell_type":"code","source":["train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","eval_dataloader = DataLoader(val_dataset, batch_size=8)"],"metadata":{"id":"ZmUysho6O8kp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_dataset))\n","len((val_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6GOjnpbz0Na","outputId":"2c48f54d-25db-4e27-cf54-cc3dbd6313a9","executionInfo":{"status":"ok","timestamp":1730007721470,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3592\n"]},{"output_type":"execute_result","data":{"text/plain":["898"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["1.Revisiting dimension requirements for Transformers in Pytorch from Chapter 3: The encoder expects data with dimensions (seq_len, batch_size). However, Hugging Face's bert-based-uncased model requires data with dimensions (batch_size, seq_len). As a result, the output from the train_dataloader has dimensions of (batch_size, seq_len)."],"metadata":{"id":"7i-QtecpxXAs"}},{"cell_type":"code","source":["item=next(iter(train_dataloader))\n","item_ids,item_mask,item_labels=item['input_ids'],item['attention_mask'],item['labels']\n","print ('item_ids, ',item_ids.shape, '\\n',\n","       'item_mask, ',item_mask.shape, '\\n',\n","       'item_labels, ',item_labels.shape, '\\n',)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhpJsZniecpV","outputId":"60009a6f-5f8a-4506-bbf2-9c9ef68dd59a","executionInfo":{"status":"ok","timestamp":1730007730232,"user_tz":-540,"elapsed":498,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["item_ids,  torch.Size([8, 512]) \n"," item_mask,  torch.Size([8, 512]) \n"," item_labels,  torch.Size([8, 2]) \n","\n"]}]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140,"referenced_widgets":["5f9ff2719215464abe8f769656975815","b7bbbf2a035b4faa98e387b55ca0bfc0","dbe653d11b0640fb8d9925cff30dc1a6","5af10585b9c44552945e43b236ba28c9","e8e1d4cfad2f4ab2ac9acdfc97a37414","dcd854374c354c9b8f05447b564eb6cc","a8bdc12eba814807b27a75f9265af2be","14b2e806d1534e8fafc6a1879bc716aa","2aeda233a7314cdbb16460d3303baea5","a7010732eb164fd98a0699bd11a042e1","9e74ed551c3a437c9a9a0ce616ecb559"]},"id":"de0AgV2QaYy4","outputId":"56196ad6-ba14-429e-d5be-bc980e88e646","executionInfo":{"status":"ok","timestamp":1730007789395,"user_tz":-540,"elapsed":12395,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f9ff2719215464abe8f769656975815"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# 5. Prepare Accelaerator\n","What is Accelerator?\n"," 1. It provides an easy-to-use API for training deep learning models on various hardware accelerators, such as GPUs, TPUs, and Apple's Metal Performance Shaders (MPS).\n","  * In our example, during training, we donot specifically select 'mps' device. THe accelerator automatically detects it and use 'mps' for training\n"," 2. The Accelerator library is particularly useful for distributed training and mixed-precision training."],"metadata":{"id":"lPDCD6Em3R5n"}},{"cell_type":"code","source":["# 모델 및 옵티마이저 준비\n","accelerator = Accelerator()\n","model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")\n"],"metadata":{"id":"GK9hKmD6OOoK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Fine Tune The Model\n","1. `lr_scheduler` in the provided code is an instance of a learning rate scheduler, which is responsible for adjusting the learning rate during the training process. The learning rate scheduler helps improve the training process by dynamically adjusting the learning rate based on the number of training steps. In this code, the learning rate starts with the initial value set in the optimizer and decreases linearly to 0 as the training progresses.\n","2. Some benefit of lr_scheduler over optimizer alone are\n","  * Faster convergence\n","  * Avoid Overshooting: When using a fixed learning rate, the optimizer might overshoot the optimal solution, especially in the later stages of training. By decreasing the learning rate over time, the model can make smaller updates and fine-tune its weights\n","  \n","3. `progress_bar` is just utility to show the progress of training\n","4. These are standard approach for fine tunning:\n","```\n"," }\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","```\n","  * each batch should be dictionary of structure {input_ids:torch.Tensor(), attention_mask: torch.Tensor(), labels: torch.Tensor()\n","  * the dimension of input_ids=(batch_size, seq_len); attention_mask= (batch_size, seq_len); and labels=(batch_size,)\n","  * You can notice that during training, we are not explicitly converting `tensor` into device; accelerator is automatically identifying the `device` and converting `tensor` into the appropriate format\n","1. After each epoch, we are also printing the evaluation metrics over the evaluation dataset"],"metadata":{"id":"e_3lyEAH34rv"}},{"cell_type":"code","source":["# 런타임 5분 30초 소요\n","\n","# 메트릭 함수 가져오기\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n","\n","num_epochs = 1\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","      \"linear\",\n","      optimizer=optimizer,\n","      num_warmup_steps=0,\n","      num_training_steps=num_training_steps\n","  )\n","progress_bar = tqdm(range(num_training_steps))\n","\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","    model.eval()\n","    #device = 'mps' 원서에 수록된 코드이나, 코랩에서는 불필요하여 주석 처리함\n","    preds = []\n","    out_label_ids = []\n","    epochs=1\n","    epoch=1\n","\n","    for batch in eval_dataloader:\n","        with torch.no_grad():\n","            inputs = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**inputs)\n","            logits = outputs.logits\n","\n","        preds.extend(torch.argmax(logits.detach().cpu(), dim=1).numpy())\n","        out_label_ids.extend(torch.argmax(inputs[\"labels\"].detach().cpu(),dim=1).numpy())\n","    accuracy = accuracy_score(out_label_ids, preds)\n","    f1 = f1_score(out_label_ids, preds, average='weighted')\n","    recall = recall_score(out_label_ids, preds, average='weighted')\n","    precision = precision_score(out_label_ids, preds, average='weighted')\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs} Evaluation Results:\")\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"F1 Score: {f1}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"Precision: {precision}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UaDCyPtoQlZR","outputId":"1b33495d-400b-48dd-9a0d-b5d18e7df391","executionInfo":{"status":"ok","timestamp":1730009228448,"user_tz":-540,"elapsed":356163,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 449/449 [09:35<00:00,  1.28s/it]\n","100%|██████████| 449/449 [05:29<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/1 Evaluation Results:\n","Accuracy: 0.9977728285077951\n","F1 Score: 0.9977729390368493\n","Recall: 0.9977728285077951\n","Precision: 0.9977829520145779\n"]}]},{"cell_type":"markdown","source":["# 6. Inference Pipeline\n","1. `tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","`: You need use the same tokenizer that was use for fine-tunning\n","2. `logits.detach().cpu()`\n","  * `detach is done to prevent  unintentional back-propogation\n","  * `.cpu` is done so that the output is compatible with scikit-learn libraries for further computation"],"metadata":{"id":"fDte1szszhiK"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","import torch\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def inference(text, model,  label, device=device):\n","    # 토크나이저 불러오기 및 입력 텍스트 토큰화\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n","    # 입력 텐서를 특정 디바이스로 전송(디폴트 값: 'cpu')\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # 모델을 eval 모드로 설정 후 추론\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","    # predicted label 인덱스 추출\n","    pred_label_idx = torch.argmax(logits.detach().cpu(), dim=1).item()\n","\n","    print(f\"Predicted label index: {pred_label_idx}, actual label {label}\")\n","    return pred_label_idx\n"],"metadata":{"id":"RKwJIhUTkja9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730009513178,"user_tz":-540,"elapsed":981,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}},"outputId":"6c75f0ad-17a1-49bf-ce35-9ec4d3f7b0e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 딥러닝 모델의 특징상 실행시 결과가 책과 다를 수 있습니다.\n","# https://abcnews.go.com/US/tornado-confirmed-delaware-powerful-storm-moves-east/story?id=98293454\n","text=\"\"\"\n","WASHINGTON (ABC) A confirmed tornado was located near Bridgeville in Sussex County, Delaware, shortly after 6 p.m. ET Saturday, moving east at 50 mph, according to the National Weather Service. Downed trees and wires were reported in the area.\n","\"\"\"\n","inference(text, model, 1.0)\n","text=\"this is definately junk text I am typing\"\n","inference(text, model, 0.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1D8y9AKRxfSS","outputId":"c3611644-708b-43c7-ef5c-6667e2ce46d5","executionInfo":{"status":"ok","timestamp":1730009590457,"user_tz":-540,"elapsed":537,"user":{"displayName":"Jason SJ Yim","userId":"00695299170620821850"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted label index: 1, actual label 1.0\n","Predicted label index: 0, actual label 0.0\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":22}]}]}