{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install the neessary packages"
      ],
      "metadata": {
        "id": "hTeOiOtpcrXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install sentencePiece"
      ],
      "metadata": {
        "id": "mfhCwAuUIdN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e10eff3-9c27-4ad5-9dc1-fdfaf7d0b629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.12.14)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.0\n",
            "Requirement already satisfied: sentencePiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Clean the Data\n",
        "1. Please Download the data from the github and save it on your local drive.\n",
        "2. At the end of this step, data should be single JSON file in the following format.\n",
        "```\n",
        "            json_entry = {\n",
        "                'instruction': 'What is diabetes?',\n",
        "                'input': '',\n",
        "                'output': 'Diabetes is ...'\n",
        "            }\n",
        "```\n",
        "3. Save the Json file in your computer."
      ],
      "metadata": {
        "id": "ly2eyDktcyAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvWpVihz0u_e",
        "outputId": "94bd52b0-9346-4e53-9086-282a44508626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 구글 드라이브 자체에 다음과 같은 폴더를 먼저 생성합니다.\n",
        "'/content/drive/MyDrive/Colab Notebooks/transformer_learn/'\n",
        "\n",
        "'/content/drive/MyDrive/Colab Notebooks/transformer_learn/dataset'"
      ],
      "metadata": {
        "id": "fccxKBdJCVpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 역자 추가 코드\n",
        "# 원본 코드에 없으나 다음 코드 추가 실행 필요\n",
        "!pip install xmltodict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O37ko1hSA8Yb",
        "outputId": "96f500cf-4367-4a77-9723-d159edb69507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ***만약 데이터가 이미 base_directory 관련 경로에 있다면 이 코드는 실행하지 말고 넘어가세요.***\n",
        "\n",
        "# 데이터를 인터넷 url에서 가져와 압축을 풀고 base_directory 관련 경로에 저장하는 과정을 포함하는 코드 블록\n",
        "# 런타임 수 분 소요\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import requests\n",
        "import xmltodict\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 기본 디렉토리 설정\n",
        "base0_directory='/content/drive/MyDrive/Colab Notebooks/transformer_learn/'\n",
        "\n",
        "# MedQuAD 데이터셋 다운로드 및 설정\n",
        "medquad_url = \"https://github.com/abachaa/MedQuAD/archive/refs/heads/master.zip\"\n",
        "medquad_zip_path = base0_directory + \"MedQuAD.zip\"\n",
        "medquad_extract_path = base0_directory + \"MedQuAD-master/\"\n",
        "\n",
        "# 데이터 다운로드\n",
        "if not os.path.exists(medquad_zip_path):\n",
        "    print(\"Downloading MedQuAD dataset...\")\n",
        "    response = requests.get(medquad_url)\n",
        "    with open(medquad_zip_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# 데이터 압축 해제\n",
        "if not os.path.exists(medquad_extract_path):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(medquad_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(base_directory)"
      ],
      "metadata": {
        "id": "s8Mgkd171jX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 코드를 실행하기 전에 MedQuAD-master 데이터셋을 아래 base_directory에 저장하는 추가 코드 필요!\n",
        "\n",
        "import xmltodict\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# base_directory를 Google Drive의 실제 경로로 설정\n",
        "base_directory = '/content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/'\n",
        "\n",
        "def convert_xml_to_json(xml_file):\n",
        "    with open(xml_file, 'r', encoding='utf-8') as f:\n",
        "        xml_data = f.read()\n",
        "\n",
        "    xml_dict = xmltodict.parse(xml_data)\n",
        "\n",
        "    # 'Document' 키가 xml_dict안에 있는지와 그 값이 None이 아닌지 점검\n",
        "    if 'Document' not in xml_dict or xml_dict['Document'] is None or 'QAPairs' not in xml_dict['Document'] or xml_dict['Document']['QAPairs'] is None:\n",
        "        print(f\"Missing or invalid 'Document' or 'QAPairs' key in {xml_file}\")\n",
        "        return []\n",
        "\n",
        "    questions = xml_dict['Document']['QAPairs']['QAPair']\n",
        "\n",
        "    # 질의(questions)가 리스트인지 확인\n",
        "    if not isinstance(questions, list):\n",
        "        questions = [questions]\n",
        "\n",
        "    json_data = []\n",
        "\n",
        "    for question in questions:\n",
        "        if question['Answer'] and question['Answer'].strip():\n",
        "            json_entry = {\n",
        "                'instruction': question['Question']['#text'],\n",
        "                'input': '',\n",
        "                'output': question['Answer']\n",
        "            }\n",
        "            json_data.append(json_entry)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "# 파일 경로 설정\n",
        "files_path = base_directory  # MedQuAD-master 폴더의 루트 경로\n",
        "\n",
        "# 모든 하위 디렉토리를 탐색하여 XML 파일 처리\n",
        "combined_json_data = []\n",
        "\n",
        "for root, dirs, files in os.walk(files_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.xml'):  # XML 파일만 처리\n",
        "            xml_file_path = os.path.join(root, file)\n",
        "            combined_json_data.extend(convert_xml_to_json(xml_file_path))\n",
        "\n",
        "# JSON 파일로 저장\n",
        "output_file = os.path.join(base_directory, 'alpaca_data.json')\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(combined_json_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"JSON 데이터가 저장되었습니다: {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTtOTF-ohKlD",
        "outputId": "77141e4a-e29e-463d-b869-dabddd6c4f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/5_NIDDK_QA/0000056.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/5_NIDDK_QA/0000064.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/5_NIDDK_QA/0000065.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/5_NIDDK_QA/0000077.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/5_NIDDK_QA/0000175.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/5_NIDDK_QA/0000177.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/6_NINDS_QA/0000007.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/6_NINDS_QA/0000018.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/6_NINDS_QA/0000182.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/6_NINDS_QA/0000244.xml\n",
            "Missing or invalid 'Document' or 'QAPairs' key in /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/9_CDC_QA/0000397.xml\n",
            "JSON 데이터가 저장되었습니다: /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/alpaca_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Declare the Model and Tokenizer\n",
        "We will utilize the lama-7b-hf model created by Meta. To obtain the model weights from Meta, you must submit a request through https://ai.facebook.com/blog/large-language-model-llama-meta-ai/. However, the Llama model's weights were inadvertently leaked and incorporated into Hugging Face's decapoda-research/llama-7b-hf. As a result, we will employ the Llama model from decapoda-research rather than requesting the weights from Meta and waiting."
      ],
      "metadata": {
        "id": "GNvQshQze-Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코랩 환경에서 다음 코드를 실행하여 토큰을 설정\n",
        "# 이 코드의 출력화면에서 허깅페이스 토큰을 입력하고 [Login] 버튼을 클릭\n",
        "# 허깅페이스 토큰 생성 과정은 [25가지 문제로 배우는 LLM 입문 with 파이썬, 임선집 외, 루비페이퍼] 책의 p132-134 참조\n",
        "# 허깅페이스의 read 토큰을 입력하면 모델 불러오기까지 무난함\n",
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "ef9f8f8028ca49b391f4565fba2f45ae",
            "1aab3f754b7e41648866a50a7430443c",
            "af96405592d148e0bde3fa8a4f242543",
            "2f81b9ea33964a84a0962037217eb37e",
            "bff1d2442f804c67aed93af6aa7c425c",
            "2d6825a12e23444aab62202aac7bf861",
            "3231b3aeb4284338bf67150b9231bfa8",
            "274a253fb2b6482f949c19897a677bd7",
            "0db2c10034d441eca2dcc731f709dcd0",
            "6c422ea4b54345eb86a0adb926a579a1",
            "2a90d06cabbf45f092d7ec3159e5261d",
            "df387a7175f547609b842da9757ca29e",
            "4d94c095122a47db8d4767a9b9aaf74c",
            "d40bf94a6bd244df8e6a40eb30f7003b",
            "c841932633894ba89454bdd84d401b4e",
            "12b9f7dd464740109e54b561ef132f88",
            "877f50751f1549d88e75108de8d3d6e0",
            "68633f9e4e984866a0ee08f117963c80",
            "44f57de477a74a94af55b6356b12c056",
            "a0768fbc379849e6b5dc1cf9df45330a"
          ]
        },
        "id": "xU1uNv3LDCiE",
        "outputId": "a4079451-6c4e-4dbb-981f-2aabc63f78b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef9f8f8028ca49b391f4565fba2f45ae"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코랩 유료 버전 기준 런타임 2분 소요\n",
        "import torch\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
        "BASE_MODEL = \"baffo32/decapoda-research-llama-7B-hf\"\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d4e64e485b9c47ddbe418fc9667d03d9",
            "596fa37856594731a8a19c7234dd18d8",
            "31071b42db1545589c846780edb98344",
            "f72073c13de24c05ab465b03484d3716",
            "bf80e660041548b9a7348bd52a80ddfb",
            "b214a75565864d4eb1e1b41d9a07fdd9",
            "3249a7f2f7914d908bd035d7a53bb154",
            "1c957f95269c4675998de1bc0759ae22",
            "a942ecc75986458c990ddd5e56bda1a6",
            "3ca90029accc4cd3b3c052085e5333fd",
            "66ee0be7cfcb43f1ba02e3d568069701",
            "1983d03b8443481c8fc20a86404b3c8c",
            "0e3b5da6c21f4e89939471f0cc36092c",
            "51dfe897b73a4cd6b00fc925198e341e",
            "2cbcf9e57c2842d48dca9bf7ad8b7f80",
            "197cfd5f887041ed8a7ef475c51380b8",
            "b91bb7fe404e45899e3e04057740321d",
            "efbbc7af808f447199ad29cc213767ab",
            "d47783c7dfd34559bc3b2ae1d2bebdde",
            "7bdba0f9355c4c38bf28999d0b5e330e",
            "4edc40eccc404ea49c9f13a7dc66101b",
            "99723960e9f247278a21c30bd28d7dfd",
            "4f303c99989c4b7190fd6f8c8afd5aa4",
            "877e257aa9ca454f8db4b66869d9e2d2",
            "c35f8564352b461c827eaa88f514f97e",
            "835fb76fadc348899d2051c012458f4a",
            "12b191dce8d449dfa8fb4de6852e8dfa",
            "84b77c18eaa740018e6e6d281091d786",
            "fafe92ec2b6d4a16a281403570112bdf",
            "f77a7f38ef4c4998b536213d14354661",
            "903bddb648c74cd38149ea8151c46585",
            "86c60da643a047138aedc2ac4bfa9b26",
            "5e0214c5ecb74bd684ce8e43c90e747d",
            "b9ce35e122d54a1d9c881c4bbcfe1b0f",
            "333d795880b1445785bdca976319f771",
            "5fc54470538e490cb0b8925b2b3b8a31",
            "82d48348093644929a7df822f96ee7ab",
            "45d9905c7f0543929a4380c160bd2af4",
            "d226cc5812b44470a89e30a8583434dc",
            "50f1633492f14312acd04a7a33999db2",
            "4458118656354572bb9b7fd2ab50966a",
            "15b0e8a10cd341038b9faf7c6721a1a5",
            "0488912f29984b26ad3a779690dc6240",
            "25f027761b9541b0a2a62c0e09dc13ed",
            "01be0043624b4c3c94c612627e003c3c",
            "1e189d8972f442f797eba6484016ce54",
            "9719a3bfee5d4d7daafc1929cca44b1a",
            "d5f5eb22a25a492fbbeae8161ae70fcb",
            "3a1c2de8488342cf88d3106c6ad2f4f3",
            "077f6e2f5b14472199da2c0c1e72d78c",
            "56cfb973baf5487a940512f6a81a8668",
            "88899d33af99450097fd297c0786993b",
            "c786fe2781b24f8bb976a6b7a1b443c1",
            "2a6fb34e4c1040d19eb2341461116af2",
            "d537483aca8a44cb902840717c4cbda0",
            "5d6600098e8f447e8b9eed61dffbcf05",
            "2b83f8c266684f2899e643bae0c2fa56",
            "ae25e70056ba45bcabbed264585b337d",
            "60d03715955249959623f7f63d1ae3e3",
            "0a65ab53141e407ab700cb4b8edd005f",
            "3acd897d55af4dc793d5b981790a2de5",
            "c28dd8c260934183a6685107f6de680c",
            "07e5bab4213d447e8ca4f2df8ad30667",
            "9599f4a0af4d4fd3867326084661005a",
            "d54b721877c04e1cb20fa2820c192623",
            "d07a071a5be846a9ba04b7ca5a91cc03",
            "dcfb148cc8b84926a1ae3738eda1c71b",
            "d73b5d971eb64b52af30325ccf607f23",
            "7a0583c9cbb74f009e1babfeabdd080e",
            "22593660ce3146b195f35c08cbf8682c",
            "cc9ef099e27148d4b2a76902a489e36e",
            "b995ecb2216d4d36890f0ce3dbf49e46",
            "02a98876980b4f45a607e349cb75c9fc",
            "701c3e85da3f4b31ad5cad2c2fbf9a83",
            "f7d1092afbce4da38861e10bf141cc4d",
            "24e4053a6ed04420bc2dc55333a48f36",
            "50ab1671808b401c990c7319aa545c2c",
            "df919e0333e94ab9aa5f810df0ceb05a",
            "8c342113bb1547e89dbf1d91169616cc",
            "1e9cb968e0eb41cc97fa5c7279d9579d",
            "f37891bb15b7406e846fd0abcc949280",
            "4f8310a5f4a94b93b29afb2fd4953804",
            "ddaa5a5e909c476c8a4f98cd24104659",
            "726a2276daa1451498794c71cc76146c",
            "33e99c1d2d19421892960e39879ca555",
            "49fa307ac0804587bd62f66fd66bf5fe",
            "dbc2fb4315a840d399441629f066126f",
            "0ac8ddd9940c4a7694ddfbab4530a530",
            "5c0b259ce28b4ee6b26559a260d3420f",
            "8c8b80457d204dc49b641865af068bea",
            "afffbf1a9b844983a6ea70c160f4c392",
            "a5898ac27ad64c47af08e585a8b0e33c",
            "b1d95bda962c4d41a91dfff18c9a87ff",
            "161901fc5d124cbfafb9dc625e5d6734",
            "605de248da6b46ea9ef1006051346fea",
            "c35020ec7b7f49028fdbaee8d6fcf76a",
            "cb8753aef08048538b0871dfb044cda7",
            "c31443b84134432e9915e72203a59b7c",
            "beef70bdcb9f44c785e6aa94b7706b5f",
            "5e57e30e65184e0aacb9e53429b1c102",
            "de5780cda63740989f9152f51bae56ec",
            "9a04c1c4f90b4e528c289868e0118ae5",
            "afbf94a2bc664f288d3e68e3896003f7",
            "7fae9e6a5f0c412bb5c1744fc2004466",
            "b9d2fbb69f614db88408adfde75db636",
            "fe6023c83a564415ab50f469820ff7a7",
            "eb5ac96fdebc4d79a1a67f9aa0496c17",
            "8a2a9fa56b734e3bb8ea0461926c7d5f",
            "7bd5e1761d9044c6bea933f1b7e9fa75",
            "b2520f99070d4b7887ac7139ea710457",
            "86c4b539be4b43c683c66954464d8c40",
            "dd3e18ab6c094ee0867801fd3b1a85d5",
            "22a7bb89639b47b493359377627123c2",
            "bc8df19b33604c5ba58d1496f27a9624",
            "4ad5e4a44aab41c4a36348076f644890",
            "1e11aa0676174cc585662613907dd869",
            "dbd0833594e64dd9a500eb21f8f93009",
            "dfa7781229b7489d86b8ac60bef57c96",
            "f52a6e09c41543a2813786cb61cc7d44",
            "a1de99c901914e9d9e82eda6ab916121",
            "31e758bb78a540c892bf1007d2f71abf",
            "4c000155236b4c749e6110c457f3f394",
            "a18eb6fd4df04b85bed1e520a0434dd0",
            "1271324d8b1a4e2a887f76565fd0f92b",
            "c3e8e84dcdd5474993297144392d1163",
            "3e6b919b3dc94e36bdc8ab0cac92bd91",
            "5a583a0215924644902dd242c7a17327",
            "20c4877a446c462fb08a2b96167af3c4",
            "c32a16a5c769483390c587c62b7d7915",
            "636c50f0876f48518d38cae6ebcbc896",
            "7f581b92492240b7971e1b509ea79bde",
            "766bdce862d24632bc8109415fac9b43",
            "9ddf680dc4e242fb9971b21d771a35e9",
            "a30e2f69296f4d4e969bd3abe826803e",
            "4719101b836a4db2864feef623e99fbc",
            "d64b218855b44b7ba227d83d046dca5a",
            "f634af4048794c4d806acff2828b1a81",
            "faa122996dcc4f078dcf2516cd984f12",
            "d152c36b3302417d94738e6765559475",
            "17701b3832944d64b05320436f3840de",
            "b6ac5b47678d41fd9d234bca3466de95",
            "4e249ce3cc2243e48f1ddca6fb807163",
            "a549abcbd8a2421182c3fd3e342b3c3b",
            "ce6185137c904d319eaf41b24c7a3d9e",
            "d40d817483394c76af2a3b33a54f5307",
            "a3c4b156c356486e9ac3e7538453d029",
            "1168a7c3fdcd4d3fb40262847a8fbd8b",
            "0e367e6679f24c8aaf5b87b54fdd5c78",
            "46346da2482544c982b303d21d2588d1",
            "f8052400d49346d3ba0471f0e2554647",
            "b5ae0dbf8b864556809aae6797071b8e",
            "74ad90a0c0c74e85b884d5914ccf159f",
            "915114f3371740e19f6ae13ed83458f6",
            "43eada6d1bc64b12be01c8638bcdccc9",
            "bb29664c00934170b51568805c3d338f",
            "cfc3b5be24f441f68697dcf638bd114d",
            "6f8133241e234df39d8f29bcc979047c",
            "f8c497fe60074739b1cedc8bcd6572a8",
            "b381683a12ba463d902232d2e3445063",
            "6f230eada42145a2a3dd121abe418336",
            "1704b06e6bab412990c9b249afc1610f",
            "89dbdf144d2d49c09acf285542a17540",
            "86ae3a69ae3c4d66af1ef032150dabc0",
            "07e97d97a6d44937b63391c16a04962d",
            "3c68402fa40740938aa1615c464228c3",
            "6334de17c5ec4f39886295790127f8b8",
            "a3115e3ff16744648a1765636ff2b9a1",
            "07b09d98eac74497bc870fc6f1c2fa16",
            "be602c77c2044adfb45bb82131d067d1",
            "c37f12f9696e404e8ff2950f28c7890a",
            "12dd9cb360294ea6893a5b5f595cca53",
            "3a0e1f9638fc43b1a5dc3747c61a1a2f",
            "c0bde77c971646849fbba56f2ab764f9",
            "692d51f6bc5044389774757d001bd77d",
            "dff6a455bc4f4d9488e97fb9cb91c4eb",
            "517e032fc19d423698a1069b1e9cbfd3",
            "325ef8e8e638494dbe441a4d63169008",
            "4b07aa68e5fe4a76bd4174c171735d34",
            "5ebcb40dabc346b3a287a7e4963c2994",
            "583cc9d20df046f292686f829acac1da",
            "04ba3a3fb9904c768bf8174e98b4504c",
            "a38c7c44c97641709d7d2655af31d5c8",
            "e5f05e326551439b96d919c3ebb648f8",
            "590843b683ea4ac885d986f623afde7c",
            "7aa5d462ebec49adb81c826ef65b13c2",
            "ff54e1d84b6e47648c5184b077f84c78",
            "fda1b0b87eb847ebbc1a10d0a0bcdc77",
            "e3d81871583547c0a5ad838c976f37fc",
            "a0e9c9a95cb34c1f81aa0571a04bbe18",
            "bc650f35c2c34dcb825d6cc9f47f2a90",
            "ab8dd8ef4fbe46eb9ef8d2fb4a27e522",
            "de440e3534cb4e3b82740b944c765d6d",
            "1e199c57f405428cb4d73ba03953be65",
            "4b64f3283b614e71a2c2f034bd1055fa",
            "bbc49fb91903464fb92976346fab5af8",
            "f5b16c834b554e8db6a1fdcbdca50b32",
            "9737fb1e3f9d44579f03a6c3d008b300",
            "e8e6017d1f044cdd935afafa7794bf63",
            "3c771f35e5234cf9937efc0805773959",
            "8dd54d7535f54090923756c70a99240c",
            "eca7a11f03a14630895ad9af11a1285f",
            "26268f9d45f04917a736b57478e9674c",
            "0171b8bbd4b1451faf1b81e5c8ae3dc6",
            "61e8e751625940b192441209f498e5f1",
            "ade993f8f63844638290ad1c696bc372",
            "54801f93b89e4b5dab848cc9b1fe7d1f",
            "f76a5492208c4a6eba28f6ace64e46ad",
            "034d31b4484141bca5310b20591daa97",
            "7aa246f1dd014e2bb594bc171ca81e7a",
            "cf76936b60464613b70985d2469d6d9f",
            "52503aceb47d462e93ab3c8365f66f07",
            "82af3967355c4ffb9df8377a84ab0996",
            "dca289caa081405496608ba935ceea23",
            "8fa610b0e5174574b4ae3e41a8f86ee0",
            "93240d0f9b854d389a4bed1f65805189",
            "7b34aace4d98474f8441c967027b52ba",
            "6913c02c9cfe4494bb1cf0daa927a715",
            "8a4a5bd9f13f45b4b0350c32f63c3f64",
            "149c7c5a29aa4031a91ede4cb58ad44f",
            "b213969ec4d34c0d89ba81f9bd2c4417",
            "b533b3963afb41c0808d032dcab57bf3",
            "df30ed5f70e44fd2adf1d2e6e816c9db",
            "68d3dc39025143dbbecfa64d6485a2cb",
            "041860326fe24dccb755b3455751983c",
            "08195bd42ae94663aedf3df239405d1e",
            "4d93e231fcd14f6fbc0dc6521729f8a3",
            "3cc63e51c3f34055a12312cc6c281711",
            "dda55fdb72504b5a80d1e81fff501a1f",
            "c2096483f602404e9e031da2f5f341f6",
            "d4b72fb7f5144c968ff7a36a9082a9f2",
            "1bc81d38be8b4cba9b093fe470212c9d",
            "b9a919e52e10459e8bce92e0c8b71f22",
            "d2c8a1f187754736b120ca15cd325c7c",
            "291d5becc3904a7098a357b621b39ca7",
            "f65397b2126e4c5ea76a4cf088a66ff2",
            "840daaa564764f86bbbda33913170335",
            "9c210f0f231b45ae8ec05bfeea65d03f",
            "7b0b2f52cada4e88a4ad4c5e36e12e5f",
            "48d75e9d112a47a880f8bb91bfdde5c3",
            "595feeb65e23480cb9b1d3e240bad5db",
            "96378aa6af9f4b418bc81deabf1f3da8",
            "3f3734988be247b0a3d4d773e823d938",
            "f0639b63fc244ccca35204d33314bb42",
            "2644f902c93e48589a7146855ab946aa",
            "bbe25c3eb2524a7aa622b313ab631515",
            "a8cc5f1ecb604c2eab6f5329ede95fee",
            "377a4bf758ae44769cd6b38d692c89e8",
            "2dbc801ad27b4441b819c08435f7a2fd",
            "f1519fde6b1d4a75a7d04178cc666153",
            "0a74260bf4ca49c1802b77e8717e9f27",
            "67e02358c442488888580a86178c4442",
            "22478bc02e37405e8fc001b68ec8bca7",
            "b9442f5524c94b6f843c8cf2279f4710",
            "e3110c4844b3475c9ec08c58bdfcfd6f",
            "d857b613d718480aad1a59f4266906de",
            "ba38f3f1d2334191a52cb8fa4ebf7143",
            "5c16a8a0dba744b086825bec71265582",
            "fb3e726ec2434ec5b22643d9f3111c25",
            "2390af741baf4406b8ed096cc68667c8",
            "8c2dfdb8b229442986253820839faa51",
            "6f75caee812c4c9fa98612d5fee65216",
            "71d68842271e4447b108e6cb32df25e9",
            "2c61e93e22fd4b60aa220156dbc80904",
            "31fb7cb384504f209406f43c01644ecc",
            "7ab7103f8fb14d92b239b6893509e5fe",
            "61ed52e71ddb40e5a6287df599b1164f",
            "7e172f76d51148938b8386f01b3f17a5",
            "4299d0ba10704a0fbd68be47b1fb441c",
            "783b395f09604b31ae02a96b54b57806",
            "9ea61046a4ba4696af8a25340c49740a",
            "fc957c3db116492c85449518f58318ed",
            "8746a9f92dc14cd1b680854ff3be9124",
            "d89fd97a2cb34e23a05d976d011c8b21",
            "f3e8c7a835914442b19da1b4f6cbad03",
            "9b8294abe6a14fef9b684aa0abdb2475",
            "cf861be223ec4dc49989cdb18c589e0a",
            "3419730b0dfc44e1a4b3ab8ebbba2835",
            "df51378ca4d0438f980f95da88485122",
            "a181ae8dd226459897154d475cd3fd35",
            "b4ff9a63c31645e58de7d2ef649d66d9",
            "32a777f2ebbf43c6bce2cc3113353caf",
            "629cc66511374b70aa92e9042225d57e",
            "f83b395302a147a4a9365da58346cb8a",
            "ec2adc16c08b4089bd9290c94d8bb839",
            "d2310d6fc2d94bfaaf889a22d80e7a18",
            "9e158a749f04406db20cee78a029e068",
            "279fe8f74de64a8e8bd16bf5bf901d81",
            "af19644b849241c8a4bd7b218221819e",
            "71bbbc2f4e1742a39ea8c985be49d564",
            "bd8e559def8144d2b0b489377b3d300a",
            "b617e49de3144c0cb8472eceeca4bd21",
            "501e6b4dcfae4181a1e2fe3de0eef58c",
            "75ae7780e8b040ef94aba92885aa601a",
            "7591891e9bc7487daf2f73207a4d4edb",
            "dedc7c3e9ea14f1581e21e3d94191cc3",
            "177a5853dfa9407fbfb866fb0110ad1d",
            "f58ea5bb64dd48a2b40b8f467082ec22",
            "46ffa3a297094c1fa0a3f9f5c07f3fec",
            "bc4e77deb0e24a7d98cb8107b55db364",
            "0fd2ebf8c0c74670b9a4f2a12bff5f79",
            "a5c6636d3bc94da4ad600ebfac2c0567",
            "cbbaffa20da74f06ae71305b81ed5df2",
            "094236b2ff3c4a5c97e387ff72ec6b7f",
            "0c4284bc318a4ee388fa2e2aed162278",
            "88cc31e369b84414aa2b333b072000be",
            "41c2fd8596ed4ee0ada6813626a37251",
            "60d0a66967ae4b008c2b2a44f20f3c4b",
            "c7c0b859db5341bf8607819c395dae63",
            "65646808e2ea4a0388ed4a0a66db304a",
            "0292946039a04b40ae75f0946e9336f7",
            "d26e5de1682f428b8626d26209b525ba",
            "8652f810580f4a068bdd276fd09e8595",
            "7dde93ba7eda4fb7aab928d864e494f4",
            "c0046012df20415b940cefbbc99f64cc",
            "03629fd2f1504839953525cc7d3d9bc1",
            "9a9cbd2b2bd048ff809a1e00c2499981",
            "a1f9fce5140b43c991fcdc34c8c2e977",
            "833a145df581428b88717a1c4724308d",
            "dda15e33ed3742778e9bebeeee224de7",
            "2d8e6012520c4ec1ac5168d19be37d36",
            "2d1b13d381d9417a9530f5d174dfb805",
            "32102cc945444df68440278ae899c237",
            "c818a9363c8f4dae91fb1a3cc1a80e67",
            "e86dbd58d01a44a286b1a8aaf903de7f",
            "39dc3186d05b4c3782890dbf824c7e69",
            "d0eebcb5badf454d922098f2e7daa67f",
            "2adaf9ad8bf949bd8897ef59f4bcd6c3",
            "0713187eba3842c58aed62022c56d0ad",
            "73d12819ea2c49ffa2925c7b564a6cd7",
            "885e11460fdf4defa862aca151d8d9f8",
            "7d61179bd5c94bbaa08e637050c7700b",
            "eea6e82cb7254042960acb1e6446463b",
            "83e7db8a4ec9422795029664734091c8",
            "19cd8263dfcc4111a12a49b47740521a",
            "cfe636dbb0384d56a67f2f2a9c8d0eef",
            "241860baeb8e4b80abaa557a8a9dd609",
            "341614e6919d4d488930fc26e3fc311e",
            "ce124a7cc262491d967a596676945281",
            "236a1e7da5b04abbb9de79004b25297f",
            "883183d713844fed8e3610118bcf3ce8",
            "168e98559c5f4fb2b13e40998104f939",
            "8dfac03a5cd9494d9a7f2845e8516b5d",
            "c82144155256432886ec615cb294cf34",
            "f58bcbfa3efd4f3cbf390666b873c3cf",
            "7064d0aed89d43e3a9a217f31ca4a01c",
            "dc369f6f6b714d3da6f26ee922926e03",
            "15aac3e87c1142c68edf8e39e68fdaf0",
            "5fd70f1286ac4b35896d23ba841853ec",
            "2863d799516c46e08b57b2314a532727",
            "538edfa0911e4a32aade4978bd0f6c9a",
            "425301938c3a4a3697e9ec2ef5282987",
            "bc1a8f7882d24970b8de5249fe710d96",
            "530c19dc1e2c418084cfed4024c177ba",
            "609eb6fcd39548c1adef7623759c58d3",
            "d8e16bdc222849808582532c22806a39",
            "682e53a7aa134d83ac965416b7d870c0",
            "320c8ca4740a440fae14d7902570d83e",
            "13575c3b42aa40db90bc13f2bf7049a7",
            "86999006b34f45b5abd60a842da0ef7f",
            "cab0056bfcfe46b8bdaf39777edee55b",
            "46c466c2fc2e4a7c891985882db678ae",
            "56e551a2242c4d8f83bd6d5748f82af3",
            "2cf3b94115524fe3bf5c8b407800cc27",
            "e323b9d08a324e5086c07aa65ddae13c",
            "de303b6c195840d1a03d339730e98a6a",
            "6c9b064aae374b078ac3ae6fae800968",
            "0fc2b08908c743b79b15e8f8ea7273d5",
            "a388ade957f44249b8d08a5be5b931a9",
            "f814d47a1bd2418fa7abeeef8e1a5455",
            "a8f0d7fe4f484e26b149b6876112a5e3",
            "33164151200d420ba9dbd62a102d2847",
            "a562ad008502452f9cc4eb8827aa403b",
            "dd24e4ab548043c8a81703cced334d54",
            "64357449a01540baa8f05b38d5a5f668",
            "1f2824b20dff41bba44508f47d802706",
            "9a89b8813ada4cae8bd1583d122d501e",
            "1aaf68f6fc984988bff89d21e6baf4b5",
            "ea27a82535714a37b8f9b8146b2c5d7e",
            "6987690c870a497ca5b9c7c70a7f83aa",
            "f6f9f0be0071405180fb4e043ce1f3fe",
            "1472f95a9cbc410f8c50d41d48e17150",
            "7dbdf0761b4740f9bab16ec60069200b",
            "1794568091a44c71a9923bc3f44756be",
            "aeecf59485204dc6956a36c1fa54bba4",
            "eb73cfe817384d75b77ce158a104d354",
            "ff3fe590d7634ae18a886b0439eca6b0",
            "20285fb8dd8f453abf1e5ce3c1a1ca50",
            "e2f01606c28b49eab28b73e24d34d015",
            "a882d28754034b178a8b05dd7cae47b7",
            "de09c7d00c79476a96655c0f704106fc",
            "2345abbb8efa4f04aa08b8482f60dfc1",
            "a8c517926fe14defb24c2aacac87a46e",
            "2f52ae189d7b435bae123162b68bd332",
            "dd29ec21e38a4c38a1e7890e0dfb05d1",
            "bd0d9f5e128644329ac0f65e395a14b9",
            "8773bd49eed94080bf9b0326bfa4bac0",
            "a2de0e2d63644738a6bef033fef2d818",
            "2271ed4c8d66434482f9933e5b868b5a",
            "927dfa7d9a3c4100978996a60738bb39",
            "2a75f93c75f043f5af74f262653b18aa",
            "184ea2733f7745d79fb82c7160beddb7",
            "90956631c92d4c2183cbdc8a4a36388f",
            "99cd9a0041a94fe9aa309c8919fdc368",
            "a3ab0a02b5444641ab7d3384afa6705a",
            "a0cc889d39624167be8587c5a0549bea",
            "538413bf1dff47a5a2439ad2723c00b9",
            "b68973f25c69470eaaa712887f709d9f",
            "b1df41513a144086b913a458ff658010",
            "b94de67b6a9d4a13a0233b09616a59dd",
            "273febbceee04b58aaefe87f30140c00",
            "da608cf64def4694b303677d71197a13",
            "a46e81a33691425ca999a9e540c5f3de",
            "f9fa2cc8243d4a619655552bb6a48468",
            "e287a83360b744dc88a212dd00424adf",
            "aa8cd9e92f0c4c5784c47673c70bd6ae",
            "27039d1f266c4ef69aa9d1854da25d55",
            "ddee73c5a1614c4abe4198dc0e6f19f7",
            "7b36ba4d8f8d42a0a2dd97f14eb191aa",
            "bfb8cef41b93433f9283eb7b02f915ef",
            "87a031ab84d541338ce28f7c1eb08272",
            "b9187ae53b3e45389807959a78bd37d7",
            "56aa38e0fc134f20a8ba98e5543b2760",
            "afdf66ca54b7476f99ced1f20d8d1c83",
            "5f7d7a86964046d197d8d535965324c3",
            "496df2de43bc461d9d46d193543c41ba",
            "a3b24a6538954609862d838391dfe6bc",
            "6244315ebf7d4c2c84f676b0b5cae62c",
            "e00571ad4d684ed2ae31b2a90648d2fa",
            "27e1cfb17d5c4fd88e164cbfe60e0310",
            "0770be908cd34001becef5e658ca2485",
            "127d3f229256425c8141f8988f7c051a",
            "6761d79cf680479ba71e76e9247fd0c8",
            "1f5b95d0a18c4d87a848d793d6df3218",
            "c52e2ac1e18e467ea98e07655ec6736c",
            "12f101ea932b4265966008d6fe3c6816",
            "7efec90e90b74480a246c78f3e5272b2",
            "28111873b02840558bf5a0e0ad51a3b1",
            "1b756efad6564dfa9562817b9ed8dc47",
            "835b3ef3dd0e498d992fd0d894a9dcea",
            "e6430db4bdd04734aea346ac3c0bad9a",
            "3347d5128f7c49cda91676aebf2682b4",
            "2e593232d4da42fcbf4ee8aa3de97fb7",
            "d9df8511f1fd475db9f914381ab47ff0",
            "649c5207276e4875a2de7e9f1149545d",
            "e03cb51dd9c9440ebdb1b3fb230aafe4",
            "acf9f49710044715934c1119aaa6dc23",
            "7f3d0a72ca404570ba4edc2ed99fe0f2",
            "3702faad757f40dbbd9f1ebed92f2abd",
            "682faec849024dfaa06695eaea804cb6",
            "a72ff1e42f61421a95673e58b3de6be5",
            "88b695e226254440935e2376dee3e949"
          ]
        },
        "id": "C3d4njzYUtWx",
        "outputId": "49b93b01-07ca-48cb-d829-4fcb7baad018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/428 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4e64e485b9c47ddbe418fc9667d03d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin.index.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1983d03b8443481c8fc20a86404b3c8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f303c99989c4b7190fd6f8c8afd5aa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9ce35e122d54a1d9c881c4bbcfe1b0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01be0043624b4c3c94c612627e003c3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00003-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d6600098e8f447e8b9eed61dffbcf05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00004-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcfb148cc8b84926a1ae3738eda1c71b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00005-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df919e0333e94ab9aa5f810df0ceb05a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00006-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c0b259ce28b4ee6b26559a260d3420f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00007-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e57e30e65184e0aacb9e53429b1c102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00008-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86c4b539be4b43c683c66954464d8c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00009-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c000155236b4c749e6110c457f3f394"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00010-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ddf680dc4e242fb9971b21d771a35e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00011-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce6185137c904d319eaf41b24c7a3d9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00012-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb29664c00934170b51568805c3d338f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00013-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6334de17c5ec4f39886295790127f8b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00014-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "325ef8e8e638494dbe441a4d63169008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00015-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3d81871583547c0a5ad838c976f37fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00016-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c771f35e5234cf9937efc0805773959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00017-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf76936b60464613b70985d2469d6d9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00018-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b533b3963afb41c0808d032dcab57bf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00019-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9a919e52e10459e8bce92e0c8b71f22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00020-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0639b63fc244ccca35204d33314bb42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00021-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3110c4844b3475c9ec08c58bdfcfd6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00022-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ab7103f8fb14d92b239b6893509e5fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00023-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf861be223ec4dc49989cdb18c589e0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00024-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "279fe8f74de64a8e8bd16bf5bf901d81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00025-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ffa3a297094c1fa0a3f9f5c07f3fec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00026-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65646808e2ea4a0388ed4a0a66db304a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00027-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d8e6012520c4ec1ac5168d19be37d36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00028-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d61179bd5c94bbaa08e637050c7700b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00029-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dfac03a5cd9494d9a7f2845e8516b5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00030-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "530c19dc1e2c418084cfed4024c177ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00031-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e323b9d08a324e5086c07aa65ddae13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00032-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f2824b20dff41bba44508f47d802706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00033-of-00033.bin:   0%|          | 0.00/524M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff3fe590d7634ae18a886b0439eca6b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2de0e2d63644738a6bef033fef2d818"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1df41513a144086b913a458ff658010"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfb8cef41b93433f9283eb7b02f915ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0770be908cd34001becef5e658ca2485"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3347d5128f7c49cda91676aebf2682b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blc2Nqstz79H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Data Preprocessing\n",
        "1. We allocate 90% of the data for training and 10% for validation purposes.\n",
        "2. The generate_prompt function establishes the prompt format. Reference: https://github.com/tloen/alpaca-lora\n",
        "  * Here, Instruction ==> Question, Input ==> Context, Output ==> Answer\n",
        "  * If there is context, the prompt will have three keys: [Instruction, Input,Output ]\n",
        "  * If there is no context, the prompt will have two keys: [Instruction,Output ]\n",
        "3. We create both training and validation datasets.\n",
        "4. Initially, we generate a prompt and subsequently tokenize it.\n",
        "5. The training process requires input_ids and attention_mask. It is not necessary to explicitly define the label.\n",
        "6. This step should produce training and validation dataset with format:\n",
        "```\n",
        "Dataset({\n",
        "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask'],\n",
        "    num_rows: 14762\n",
        "})\n",
        "```"
      ],
      "metadata": {
        "id": "8LNQXA66z-Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코랩 유료 버전 기준 런타임 1분 소요\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = load_dataset(\"json\", data_files=base_directory+\"alpaca_data.json\", split=\"train[:90%]\")\n",
        "valid_data = load_dataset(\"json\", data_files=base_directory+\"alpaca_data.json\", split=\"train[90%:]\")\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    if data_point[\"input\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### 지시(Instruction):\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### 입력(Input):\n",
        "{data_point[\"input\"]}\n",
        "\n",
        "### 출력(Response):\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### 지시(Instruction):\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### 출력(Response):\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "train_data1 = load_dataset(\"json\", data_files=base_directory+\"alpaca_data.json\", split=\"train[:90%]\")\n",
        "valid_data2 = load_dataset(\"json\", data_files=base_directory+\"alpaca_data.json\", split=\"train[90%:]\")\n",
        "data_train = train_data1.shuffle().map(\n",
        "    lambda data_point: tokenizer(\n",
        "        generate_prompt(data_point),\n",
        "        truncation=True,\n",
        "        max_length=1000,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        ")\n",
        "data_valid = valid_data2.shuffle().map(\n",
        "    lambda data_point: tokenizer(\n",
        "        generate_prompt(data_point),\n",
        "        truncation=True,\n",
        "        max_length=1000,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "16063ae962d34d629404fdd881028bee",
            "60a5bc9aebfb4135b090ac06e1f04ab0",
            "ce4d66e09b484aa58950a39c404f9ddd",
            "995df686702f42cd9de47348108d5f8b",
            "0fd4f00b0d314e17827c3b02cec223b4",
            "fd8b46328d38427fbf2dfa00ae483d9a",
            "37d249e866a94c179867d042965e03aa",
            "132bb344e6fc4917bdecddb70ec9c8fe",
            "fa75f4583fed49fbaf2518a63c3b4f2a",
            "3903ccb64051488fb205440fe73807e2",
            "1139c9ece5cd47b7927ab9eae842af16",
            "9ab786a0f47447a0a430d540b44b06b4",
            "fe26224cde2c4f17bbe4acaa761b8e52",
            "efe201b3684f4fbe90d0c0e67a41c836",
            "a24962af459b4450bd0c509b7f54d5d3",
            "d6de937bb91040849ef19432a46edd1c",
            "90a58632b4064911808855d3d7f5ac81",
            "529fdc79203948e98885f005f6531a35",
            "b67d007769ab40228ee0f16e0e4ff74a",
            "cb275f32598540c689d439d178c9f35c",
            "0ee00ffec4344aef89dcc76545485a43",
            "84bfa21252684cef8f820beac84363f7",
            "acaa86f60a8a4cc98a2ecdae402a7188",
            "2ca2734789e54c22947e2ffddf1d3b30",
            "69503767f2ef40dea6fbf41b3334f007",
            "fad6082d7ff34fd8bf7f6b11fc1620cc",
            "456ddce277074d51a7ffb71d01024dd6",
            "f602f9225ee940e0b436355aa127dac0",
            "7f7f67712948400a9fa76f761f3bf4f7",
            "6348ce910f9942e18377a7e775ac317a",
            "d7b7388a14df463f989b8b6aa0e16479",
            "1edef6861a7f4ee3b4389ebdb7958050",
            "be88697271344a019ddbf2da0fbd4ca3"
          ]
        },
        "id": "YDApdOlEjCDq",
        "outputId": "27944ab1-3644-4e85-9855-ce8c539d615d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16063ae962d34d629404fdd881028bee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14762 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ab786a0f47447a0a430d540b44b06b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1640 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acaa86f60a8a4cc98a2ecdae402a7188"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 역자 추가 코드\n",
        "# 저장 경로 설정\n",
        "data_train_save_path = os.path.join(base_directory, \"data_train_saved\")\n",
        "data_valid_save_path = os.path.join(base_directory, \"data_valid_saved\")\n",
        "\n",
        "# 데이터셋 저장\n",
        "data_train.save_to_disk(data_train_save_path)\n",
        "data_valid.save_to_disk(data_valid_save_path)\n",
        "\n",
        "# 저장 경로 출력\n",
        "print(f\"data_train saved to: {data_train_save_path}\")\n",
        "print(f\"data_valid saved to: {data_valid_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "59c398e71b544e57a5260db2c1c3521d",
            "058a12fbce5d485cbdcf7aef440096d3",
            "574afe7daabd4d379a7a5cd7d691987f",
            "08c8f77607e4468098af6aed21202351",
            "6eb4601b06f14ebfa97c7e16b6a6b08e",
            "b42a44fcf8824c92bd4e2b71e2c18fc4",
            "3c2b3fead4544ebcb7354a8d65dfafc7",
            "5cabe85f2bb2443e8d5924cb84801b17",
            "9d2ebe467b814c07b36b757461aeda39",
            "9d64a776e45b4940922cfbf01400f378",
            "b72209cf00ed4342a39e8f5c819fe66f",
            "ddb7dde4d71d4dd6a99e77d0e42bf954",
            "6a834f3b6e974c3b8baa619a96d41127",
            "507b6c26c21744269a8c863076e80ac5",
            "4c4b5f0f262a4090b3ac99cccc9ec0cc",
            "8e97585d346d413798614bd0831be694",
            "090913d451174244a8dfceba83ce8cbe",
            "b9484d15732a43bf879773de38847558",
            "8d1d4d6d2d1c40adb33644df5570661e",
            "5c320523bdb748faa010e087f15afe0b",
            "f8aa68a0e9b04277ae80db09ab100c84",
            "ca95278768034e309998dc36d254c0e0"
          ]
        },
        "id": "d2-ktmjWrZPD",
        "outputId": "fdef41a3-8ed9-4e45-9b4a-3e49cab31b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/14762 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59c398e71b544e57a5260db2c1c3521d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1640 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddb7dde4d71d4dd6a99e77d0e42bf954"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_train saved to: /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/data_train_saved\n",
            "data_valid saved to: /content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/data_valid_saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training With PEFT"
      ],
      "metadata": {
        "id": "IkU6HyHT0FdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delaring Lora Variables"
      ],
      "metadata": {
        "id": "203sk2Kn0KcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LORA_R = 8 # Lora 차원\n",
        "LORA_ALPHA = 16 # Lora 스케일링(scaling)용 alpha 파라미터\n",
        "LORA_DROPOUT= 0.05\n",
        "# 학습될 파라미터 정의\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "]\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "MICRO_BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "LEARNING_RATE = 4e-4\n",
        "TRAIN_STEPS = 50\n",
        "OUTPUT_DIR = base_directory"
      ],
      "metadata": {
        "id": "FcDlLV6njCHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Below output shows that we are only training 0.06 percentage of parameter, which will higly spped-up fine-tunning process"
      ],
      "metadata": {
        "id": "qqKDBS0e0YSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 역자 주: 최신 버전의 peft 라이브러리에서는 prepare_model_for_int8_training 함수가 prepare_model_for_kbit_training으로 변경되었습니다.\n",
        "###        때문에 원문 코드를 아래와 같이 변경합니다.\n",
        "#from peft import LoraConfig, get_peft_model, get_peft_model_state_dict, prepare_model_for_int8_training\n",
        "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "#model = prepare_model_for_int8_training(model)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a7wM7HcjCLs",
        "outputId": "ccd230c0-43d8-42f2-89ed-65c0bfccc71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "training_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=10,\n",
        "    max_steps=TRAIN_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_torch\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBSBXe8yjCO1",
        "outputId": "68ca8e54-8255-42a3-932d-0be55f238cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and save the chatbot"
      ],
      "metadata": {
        "id": "XzsqlS-RZg7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 역자 주: 다음 원서 코드는 전체 과정의 1/10을 진행하는데도 2시간 정도 소요됩니다.\n",
        "###        따라서 이 코드 대신에 더 단순화된 다음 코드 블록 실행을 권장합니다.\n",
        "\"\"\"\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data_train,\n",
        "    eval_dataset=data_valid,\n",
        "    args=training_arguments,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "model.save_pretrained(base_directory+\"chatbot\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Pl0z_w1GlZU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 역자 추가 코드: 학습 시간을 대폭 축소시키는 코드\n",
        "### 런타임 1시간 이내 (유료 코랩 Colab PRO 기준)\n",
        "\n",
        "LORA_R = 8  # lora dimension\n",
        "LORA_ALPHA = 16  # (`float`): The alpha parameter for Lora scaling\n",
        "LORA_DROPOUT = 0.05\n",
        "# this defines what parameters need to be trained\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "]\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "MICRO_BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "LEARNING_RATE = 4e-4\n",
        "TRAIN_STEPS = 50  # 기존 설정: TRAIN_STEPS = 50\n",
        "OUTPUT_DIR = base_directory\n",
        "\n",
        "### 역자 주: 최신 버전의 peft 라이브러리에서는 prepare_model_for_int8_training 함수가 prepare_model_for_kbit_training으로 변경되었습니다.\n",
        "###        때문에 원문 코드를 아래와 같이 변경합니다.\n",
        "#from peft import LoraConfig, get_peft_model, get_peft_model_state_dict, prepare_model_for_int8_training\n",
        "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# model = prepare_model_for_int8_training(model)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "import transformers\n",
        "\n",
        "# 기존 TrainingArguments\n",
        "# training_arguments = transformers.TrainingArguments(\n",
        "#     per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "#     gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "#     warmup_steps=10,\n",
        "#     max_steps=TRAIN_STEPS,\n",
        "#     learning_rate=LEARNING_RATE,\n",
        "#     fp16=True,\n",
        "#     logging_steps=10,\n",
        "#     optim=\"adamw_torch\",\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     save_strategy=\"steps\",\n",
        "#     eval_steps=50,\n",
        "#     save_steps=50,\n",
        "#     output_dir=OUTPUT_DIR,\n",
        "#     save_total_limit=3,\n",
        "#     load_best_model_at_end=True,\n",
        "#     report_to=\"tensorboard\"\n",
        "# )\n",
        "\n",
        "# 수정된 TrainingArguments (효율성 개선)\n",
        "training_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=2,  # 배치 크기 축소 (기존: 4)\n",
        "    gradient_accumulation_steps=16,  # 더 큰 Gradient Accumulation\n",
        "    warmup_steps=5,  # 워밍업 단계 감소\n",
        "    max_steps=25,  # 학습 단계를 줄임 (기존: 50)\n",
        "    learning_rate=2e-4,  # 학습률 감소 (안정성 증가)\n",
        "    fp16=True,  # FP16 유지\n",
        "    logging_steps=5,  # 로깅 빈도 증가\n",
        "    optim=\"adamw_torch\",\n",
        "    evaluation_strategy=\"no\",  # 평가 비활성화 (학습 속도 증가)\n",
        "    save_strategy=\"no\",  # 체크포인트 저장 비활성화 (학습 속도 증가)\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    report_to=\"none\",  # TensorBoard 비활성화\n",
        ")\n",
        "\n",
        "# 기존 Trainer\n",
        "# trainer = transformers.Trainer(\n",
        "#     model=model,\n",
        "#     train_dataset=data_train,\n",
        "#     eval_dataset=data_valid,\n",
        "#     args=training_arguments,\n",
        "#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "# )\n",
        "\n",
        "# 수정된 Trainer (더 작은 데이터로 학습)\n",
        "small_data_train = data_train.select(range(500))  # 학습 데이터 크기 축소\n",
        "small_data_valid = data_valid.select(range(100))  # 검증 데이터 크기 축소\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=small_data_train,  # 축소된 데이터 사용\n",
        "    eval_dataset=small_data_valid,\n",
        "    args=training_arguments,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# 기존 모델 캐시 비활성화\n",
        "model.config.use_cache = False\n",
        "\n",
        "# 학습\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "# 모델 저장\n",
        "model.save_pretrained(base_directory + \"chatbot\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "USfwsxzblZYf",
        "outputId": "728caae9-891e-4e91-ba74-d6bea602a5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 58:40, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.528600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.449900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.274400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.452700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.126400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Model for inference"
      ],
      "metadata": {
        "id": "c14cf4-UlxnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process"
      ],
      "metadata": {
        "id": "xGVXzo4r0j8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "YOMlAtdOMBTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "from peft import PeftModel, PeftConfig\n",
        "import torch\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "# Base directory 설정\n",
        "base_directory = '/content/drive/MyDrive/Colab Notebooks/transformer_learn/MedQuAD-master/'\n",
        "\n",
        "# 모델 경로 설정\n",
        "BASE_MODEL = \"baffo32/decapoda-research-llama-7B-hf\"\n",
        "peft_model_id = base_directory + \"chatbot\"\n",
        "\n",
        "# PEFT Config 불러오기\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "# Base 모델 불러오기(float16 사용 및 자동 장치 설정)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"  # 자동으로 GPU 또는 CPU 할당\n",
        ")\n",
        "\n",
        "# PEFT 모델 불러오기(메모리 부족 방지)\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    peft_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_buffers=True  # 버퍼를 오프로드하여 GPU 메모리 부족 방지\n",
        ")\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# GPU에 모델 업로드\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA가 사용 불가능합니다. 모델을 CPU에 올립니다.\")\n",
        "    model = model.to(\"cpu\")\n",
        "\n",
        "# 모델 평가 모드 설정\n",
        "model.eval()\n",
        "\n",
        "# 입력 텍스트 예시\n",
        "input_text = \"Hello, how can I assist you today?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 텍스트 생성\n",
        "outputs = model.generate(input_ids, max_length=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "S6fAmiADlZg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "c0379734b8ed4f28a84edf01f4d15b24",
            "b6377a22c29f44c1abdc688ef616b9bb",
            "d1e29293b3844e0bb9dbccd6ab40af51",
            "6e38c022aaaf4615bdb4fb9457b26a3b",
            "0c26eb9ba96744c7bd27d5511d974b24",
            "e611534dd81c4f40935ad9a4cc066fd1",
            "84b31061eb8347c4a2493b013fb1514d",
            "2ef3c2d4315d413ca6fac26e690872c7",
            "750c6d5cb90c47658beb1bab91620a4c",
            "ab69e83179034504a7dd9002bcd08f23",
            "b236ee1f5dd046caa70cb1b2dee5a512"
          ]
        },
        "outputId": "d3e58854-371e-469c-8fcb-4a7dbdda8efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0379734b8ed4f28a84edf01f4d15b24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how can I assist you today?\n",
            "I'm calling to cancel my account.\n",
            "I'm calling to cancel my account. I'm not happy with the service.\n",
            "I'm calling to cancel my account. I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Function"
      ],
      "metadata": {
        "id": "I0VY3waQl7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE='cuda'"
      ],
      "metadata": {
        "id": "FimYoknnl5TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from peft import PeftModel\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
        "\n",
        "def ask_ai_doctor(instruction: str, model: PeftModel) -> str:\n",
        "    PROMPT_TEMPLATE = f\"\"\"\n",
        "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    [INSTRUCTION]\n",
        "\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "\n",
        "    # 템플릿(template)의 [INSTRUCTION]을 주어진 instruction으로 대체\n",
        "    prompt = PROMPT_TEMPLATE.replace(\"[INSTRUCTION]\", instruction)\n",
        "\n",
        "    # 프롬프트를 입력 텐서로 인코딩\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
        "\n",
        "    # 생성 configuration 설정\n",
        "    '''\n",
        "    controls various aspects of the text generation process.\n",
        "    temperature: This parameter (set to 0.1) controls the randomness of the generated text. lower value more determenistic; higher value more random\n",
        "    top_p: This parameter (set to 0.75) is also called nucleus sampling. In our case, the model will only consider tokens that make up the top 75% of probabilities for the next word\n",
        "    repetition_penalty: This parameter (set to 1.1) is used to penalize repetitions in the generated text. A value greater than 1 helps to reduce the frequency of repeated phrases\n",
        "    '''\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    # 모델과 configuration을 사용하여 응답 생성\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        response = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=250,\n",
        "        )\n",
        "\n",
        "    # 디코딩된 관련 응답을 추출\n",
        "    decoded_output = tokenizer.decode(response.sequences[0])\n",
        "    formatted_response = decoded_output.split(\"### Response:\")[1].strip()\n",
        "\n",
        "    # wrap을 사용하여 포맷된 응답 출력\n",
        "    return \"\\n\".join(textwrap.wrap(formatted_response))\n"
      ],
      "metadata": {
        "id": "TcwAkOFjl5WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 역자 주: 코랩 메모리 제약상 원서와 달리 모델과 데이터셋을 간략화했기 때문에 결과가 책과 다를 수 있습니다.\n",
        "print(ask_ai_doctor('What are symptoms of Cirrhosis?', model))"
      ],
      "metadata": {
        "id": "BqIp7SuVl5Zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9240cf4f-7bc1-4328-e4d5-24ebe939cabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symptoms of cirrhosis include:         * Jaundice (yellowing of skin\n",
            "and eyes)         * Abdominal pain         * Weight loss         *\n",
            "Fatigue         * Nausea         * Itchiness         * Swelling in\n",
            "legs, feet, abdomen, or scrotum         * Bleeding from veins\n",
            "* Easy bruising         * Sensitivity to light         * Muscle\n",
            "weakness         * Confusion         * Loss of appetite         *\n",
            "Drowsiness         * Constipation         * Menstrual irregularities\n",
            "* Impotence         * Hair loss         * Skin rashes         *\n",
            "Pruritus (itchy skin)         * Fluid retention         * Ascites\n",
            "(fluid buildup in abdomen)         * Encephalopathy (brain damage)\n",
            "* Hepatomegaly (enlarged liver)         * Spider angiomas (red spots\n",
            "on skin)         * Portal hypertension (high blood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 원서 코드의 최종 결과물 (메모리 제약 없이 원서 코드대로 실행했을 때의 결과물)\n",
        "\"\"\"\n",
        "print(ask_ai_doctor('What are symptoms of Cirrhosis?', model))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gwNayFWI_eE",
        "outputId": "8d337b53-eca3-48d3-d523-8fc9a16b5d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following list of signs and symptoms may be associated with\n",
            "cirrhosis.  Some people with cirrhosis do not have any of these\n",
            "symptoms.   If you are concerned about how your general health is\n",
            "affected by cirrhosis, talk to your doctor or nurse practitioner.\n",
            "Signs and Symptoms of Cirrhosis   ------------------------   Abdominal\n",
            "swelling (ascites)   Bleeding problems   Blurred vision   Breath odor\n",
            "Confusion   Constipation   Difficulty concentrating   Dizziness\n",
            "Fatigue   Fluid retention   Gallstones   Gout   Hair loss   Headache\n",
            "Itching   Jaundice   Liver cancer   Memory loss   Muscle weakness\n",
            "Nausea   Neuropathy   Night sweats   Pain in the upper right abdomen\n",
            "Poor appetite   Skin itching   Sleepiness   Stomach pain   Swollen\n",
            "legs and feet   Tiredness   Weight gain   Yellow skin and eyes\n",
            "------------------------   How common are these symptoms?   These\n",
            "symptoms can occur at different times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 이하 코드는 단순 참조 코드"
      ],
      "metadata": {
        "id": "hhafFQ5k1w8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "oujRqT9IXDFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "eb1ce7e0246a44f4901b8d210d1637bf",
            "22c3444491cd46c2b90c10ae51617e2b",
            "e2a9a588928948ea80f8edd67de5c5b3",
            "5ef68a4243bc48788ac4fafaeb2b06f9",
            "be46b5f6504a4690a1ca349b925f994e",
            "b48afc0b489946248cf40968060dbc58",
            "7c12e18930e64815aa6abde50b62b933",
            "6198c07cfb68472a9eb44d64f12ecec4",
            "069acec41bd04bb7a77cf63928d1bd8b",
            "a68baafded5a4f7a8f729b30b92bf025",
            "48356b00d03541abbd13e67a730870ff",
            "2ec7fbc6fbda4259a02a8876402560c5",
            "962566eca9f6488890fd54958b255efe",
            "3d7c6b25113b4f7e85bb968d7f5720d9",
            "84977d4c8a2d44e980eb9878e8110ae3",
            "8badd5e2a7eb46be8bea6671ba18076c",
            "93a64672268542649288fb4b4f07c5bb",
            "d877d24fb8f14182811ccc33415a4fbf",
            "da7670d7cb7b46e8b043411d6b3b0241",
            "9dc0b7bab53d4e36bb3d04b1d4bb82bd",
            "d39410a55b5742779daecd85321b532e",
            "46dc52d4704240d3aa0d30d2463913e6",
            "b3c45ad692c148d493f080c746c3eb1c",
            "bae94ba76e3c4ec08f41339a4006c164",
            "e12e1768d62a4f1b993c60dd57f080f3",
            "ab1154f47b994696bd3725b14476038d",
            "d2d553c73df140e5b0db0533812809f8",
            "9aeb40bd4dfd4c318769ddb2041f2c3d",
            "e49f3872fffa4528984e2eff3a5a1628",
            "0f6778469f434cad9b12deb936feb7d4",
            "8bdee943d5ac4bb4bdb227ccba504666",
            "27481e5242da401abf36cce1b9dd4b08"
          ]
        },
        "id": "ztb31ixajI3P",
        "outputId": "3349d977-d0f9-4464-cee6-de1c121da801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb1ce7e0246a44f4901b8d210d1637bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 모델명 및 허깅페이스 경로는 원저자의 것이므로 단순 참조\n",
        "model.push_to_hub(\"prem-timsina/alpaca-ai-doctor\", use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135,
          "referenced_widgets": [
            "8797e5ab6f034daa858807c33fc239be",
            "e4820ccf98344bf380844ec7ebad199a",
            "36f25b3f469c49a88aed627363bf4f34",
            "0289724256af414d95b4ad154dfbc0f8",
            "34e5115eb16c4b828870d947f4301254",
            "b370206610ee4b32a98fbb55954347bf",
            "ddd70b24cccf4d04b600ec5ded701916",
            "cbb3d68654bb4688b9836e627c45f3d2",
            "a34a9315491943f38738a277ba395368",
            "9652fa8f96ec4730b673bc362b46905a",
            "b306cffd5fa44594b1e495ee5bc4d27a",
            "21f9de5110314e44a35b4cd80ddad7c9",
            "c7f41f6526b94702af10a47ee48984c9",
            "8450d8fdf7674647a499065c9d37c00f",
            "c41fcb80ea2649059396946108e194c1",
            "da2fdb74b9644c35af98abdb422713b0",
            "5e54c1a819ed497f82d4de16bdd1497d",
            "40196715379b4e269ccca4e5e120af1b",
            "5c47009656ad40dc9ff7e9bd5e5c2d16",
            "456c1763363843b3b4e793e69b429e70",
            "b41ed183871a4ddda1bd0285b3511d0c",
            "894fcbbd987c4a448b9ef31b41f6371b"
          ]
        },
        "id": "CZ2QrGREjOIs",
        "outputId": "94a723da-ea81-4887-dfa6-76a9557c998f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8797e5ab6f034daa858807c33fc239be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.bin:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21f9de5110314e44a35b4cd80ddad7c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/prem-timsina/alpaca-ai-doctor/commit/be92fbce9e96a94dc5e8d3cbaed190acc7c03462', commit_message='Upload model', commit_description='', oid='be92fbce9e96a94dc5e8d3cbaed190acc7c03462', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fD7uKU3rrNM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}