{"cells":[{"cell_type":"markdown","source":["#원서에서는 Ch17에 별도의 ipynb 노트북 파일을 제공하지 않습니다.\n","#여기서는 독자 여러분의 편의를 위해 Ch17에서 언급한 코드를 입력해 두었습니다."],"metadata":{"id":"buGibdD6rbD2"},"id":"buGibdD6rbD2"},{"cell_type":"code","execution_count":null,"id":"RFW2ck1dYgNi","metadata":{"id":"RFW2ck1dYgNi"},"outputs":[],"source":["### 이하 Ch17의 본문에서 언급된 코드들입니다.\n","### 서로 연결되어 실행되지 않습니다.\n","### 단순하게 코드 자체만 참조하시기 바랍니다.\n","\n","special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n","num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"lrNguH-mYgRM","metadata":{"id":"lrNguH-mYgRM"},"outputs":[],"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 예제 문장\n","sentences = [\"Hello world!\", \"Attention masks are important.\"]\n","encoded_input = tokenizer(sentences, padding='max_length', truncation=True, max_length=10, return_attention_mask=True)\n","\n","print(encoded_input['input_ids'])\n","print(encoded_input['attention_mask'])\n"]},{"cell_type":"code","source":["data_collator = DataCollatorWithPading(tokenizer=tokenizer)"],"metadata":{"id":"-7Al5h8YiGVU"},"id":"-7Al5h8YiGVU","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7-jQQJzJYgUj","metadata":{"id":"7-jQQJzJYgUj"},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n","from torch.utils.data import Dataset\n","\n","# 토크나이저 불러오기\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 데이터 준비\n","sentences = [\"Hello world!\", \"I love machine learning.\", \"Transformers are powerful.\", \"HuggingFace is great for NLP tasks.\"]\n","labels = [0, 1, 1, 0]\n","\n","# 패딩 및 텐서로 변환 조치 없이 토큰화\n","encodings = tokenizer(sentences, truncation=True, padding=False, return_tensors=None)\n","\n","# CustomDataset 함수 정의\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","      return len(self.labels)\n","\n","dataset = CustomDataset(encodings, labels)\n","\n","# 모델 초기화\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# 동적 패딩을 위한 Data Collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# Training Arguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=2,\n","    logging_dir='./logs',\n","    logging_steps=1,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=1,\n","    save_strategy=\"steps\",\n","    save_steps=1,\n","    no_cuda=False,\n","    output_dir=\"./results\",\n","    overwrite_output_dir=True,\n","    do_train=True\n",")\n","\n","# Trainer 초기화\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    data_collator=data_collator\n",")\n","\n","# 학습\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"id":"pVi201XxYgXZ","metadata":{"id":"pVi201XxYgXZ"},"outputs":[],"source":["print('dataset[0]',dataset[0]['input_ids'])\n","print('dataset[1]',dataset[1]['input_ids'])\n"]},{"cell_type":"code","execution_count":null,"id":"Z528YXxCYgaX","metadata":{"id":"Z528YXxCYgaX"},"outputs":[],"source":["# 포워드 패스\n","output = model(input_tensor)\n","loss = loss_fn(output, target_tensor)\n","\n","# 백워드 패스\n","optimizer.zero_grad()\n","loss.backward()\n","\n","# 기울기 클리핑(Gradient Clipping)\n","torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","# 옵티마이저 step\n","optimizer.step()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"qIWtgG6AYgdi","metadata":{"id":"qIWtgG6AYgdi"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","source":["assert tensor.device == next(model.parameters()).device, \"Tensor and model parameters must be on the same device\""],"metadata":{"id":"jy8ASrTJwyri"},"id":"jy8ASrTJwyri","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":5}