{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE8tTD1OMJCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "executionInfo": {
          "status": "ok",
          "timestamp": 1735300172818,
          "user_tz": -540,
          "elapsed": 15256,
          "user": {
            "displayName": "Jason SJ Yim",
            "userId": "00695299170620821850"
          }
        },
        "outputId": "d7758632-e864-4fb2-a130-bc6ac78c9d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
            "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: captum\n",
            "Successfully installed captum-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 런타임 30초 소요\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from captum.attr import LayerIntegratedGradients, visualization as viz\n",
        "import torch\n",
        "\n",
        "def visualize_sentiment(text: str):\n",
        "    \"\"\"\n",
        "    Visualizes the sentiment of the given text using a pre-trained DistilBERT model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to visualize.\n",
        "    \"\"\"\n",
        "\n",
        "    # 사전 학습 모델 및 토크나이저\n",
        "    model_path = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    # 주어진 텍스트에 대해 입력 텐서와 베이스라인을 생성하는 함수 정의\n",
        "    def construct_input_and_baseline(input_text: str):\n",
        "        \"\"\"Constructs input and baseline tensors for the given text.\"\"\"\n",
        "        max_length = 768\n",
        "        baseline_token_id = tokenizer.pad_token_id\n",
        "        sep_token_id = tokenizer.sep_token_id\n",
        "        cls_token_id = tokenizer.cls_token_id\n",
        "\n",
        "        text_ids = tokenizer.encode(input_text, max_length=max_length, truncation=True, add_special_tokens=False)\n",
        "        input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "        baseline_input_ids = [cls_token_id] + [baseline_token_id] * len(text_ids) + [sep_token_id]\n",
        "        token_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "        return torch.tensor([input_ids], device='cpu'), torch.tensor([baseline_input_ids], device='cpu'), token_list\n",
        "\n",
        "    # 입력과 베이스라인 구축\n",
        "    input_ids, baseline_input_ids, all_tokens = construct_input_and_baseline(text)\n",
        "\n",
        "    # 모델 출력 함수 정의\n",
        "    def model_output(inputs):\n",
        "        return model(inputs)[0]\n",
        "\n",
        "    # 층 적분 그레이디언트\n",
        "    lig = LayerIntegratedGradients(model_output, model.distilbert.embeddings)\n",
        "\n",
        "    # 타깃 클래스\n",
        "    target_classes = [0, 1]\n",
        "    attributions = {}\n",
        "    delta = {}\n",
        "\n",
        "    # 클래스 속성(attributions) 계산\n",
        "    for target_class in target_classes:\n",
        "        attributions[target_class], delta[target_class] = lig.attribute(\n",
        "            inputs=input_ids,\n",
        "            baselines=baseline_input_ids,\n",
        "            target=target_class,\n",
        "            return_convergence_delta=True,\n",
        "            internal_batch_size=1)\n",
        "\n",
        "    # 속성(attributions) 요약\n",
        "    neg_attributions = attributions[0].sum(dim=-1).squeeze(0) / torch.norm(attributions[0])\n",
        "    pos_attributions = attributions[1].sum(dim=-1).squeeze(0) / torch.norm(attributions[1])\n",
        "\n",
        "    # 클래스 예측\n",
        "    pred_prob, pred_class = torch.max(\n",
        "        model(input_ids)[0]), int(torch.argmax(model(input_ids)[0]))\n",
        "\n",
        "    # 예측된 클래스에 근거하여 속성 선택\n",
        "    summarized_attr = pos_attributions if pred_class == 1 else neg_attributions\n",
        "\n",
        "    # 데이터 시각화\n",
        "    score_vis = viz.VisualizationDataRecord(\n",
        "                        word_attributions=summarized_attr,\n",
        "                        pred_prob=pred_prob,\n",
        "                        pred_class=pred_class,\n",
        "                        true_class=None,\n",
        "                        attr_class=text,\n",
        "                        attr_score=summarized_attr.sum(),\n",
        "                        raw_input_ids=all_tokens,\n",
        "                        convergence_score=delta[pred_class])\n",
        "\n",
        "    # 결과 시각화\n",
        "    viz.visualize_text([score_vis])\n"
      ],
      "metadata": {
        "id": "vThfBIUdT5mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 런타임 20초 소요\n",
        "text = \"The movie was not bad as mentioned by critics. It was in fact awesome; I enjoyed the whole time\"\n",
        "visualize_sentiment(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398,
          "referenced_widgets": [
            "e32a2f8739db4cf38a8abce4cd8cc8b0",
            "18a2b6fb13204602b9992eb3811724fb",
            "ba08436e58f74497a66675b702578e1d",
            "a38c949fc5a44134908d706d3e78463c",
            "41243974358e47fca4405128d449196b",
            "82c95e82215241c6842fdb85ff49356b",
            "018a51c79a86429b8aaf6983abcb13b7",
            "04e54c82e24149348e9e2c1c4141bfc3",
            "28d29a61eff5462c84cde5205cbfca7f",
            "963c57ca6ebc4f36b0747a424e924fdf",
            "bcb907a40c1c4e9382872924e49a5d0c",
            "ac2a1934b54a42678bacb2c27df436b6",
            "bc2844857e6643dfa2d855be0015c7c6",
            "55b0c0784cd84184969be85c01e6716b",
            "bada800befc142c381b92510e3bbaf1f",
            "51a3df8123ee42b5a0fd7d14cff1e11f",
            "5efe007277614feab70e7397efbb538b",
            "0ddd868c45bf446bae945a9861ecc6b0",
            "d501e82e33f74fe3b3c10785ab4c6ede",
            "8958d8b03620423798ef8cc33e6bc5bb",
            "102c6d8fbdb942c4bdf2c4e6d19f5180",
            "525fab83f20043a8a596ce08bf0f2079",
            "c3dcb8bb9f3a491aaa5bc21ff5722384",
            "69f7f99800984c138cd44e6617d20041",
            "7cdf69581af04c229c13da6d7a503229",
            "f77073399f42493aa92e132278b4d6f5",
            "fede022e884b42c2914c954cec980b65",
            "23a89d92709044368f0875d94db6d4a0",
            "fd483800e4fb434a8bfd8856279898f9",
            "dc14a92e8f40450abe404f6095e89931",
            "75af83f12800456793b760c8e36e001f",
            "3c017aee21974d99b20af51f70cdd655",
            "f341466812be4e9f8a7d6f511c682448",
            "fc4b9faf44484434ab068aef180cae6d",
            "e5a4d536da7d4db98ab2a9c1a8bb26d1",
            "a28e2da59cd54ef8a7056cf4ff92b297",
            "83299a477d114b3aa905a371d6f77132",
            "05a257d9165c4ecc92b108c9b084f757",
            "0cb28eea100f46fcbc030bb219260d18",
            "c2835da7156a4b92851aed58847210f9",
            "000b44e7cde94534925eda5e8add02e0",
            "10a0ff61764f45928b28d3c1e587c088",
            "935cd059fa474c6ab529c99f735c9303",
            "314b300e46204d1cbf70695f87853b6b"
          ]
        },
        "id": "VlYR7OK9WbxC",
        "outputId": "1ce25a18-8f51-4a6f-a141-603e947f6da9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1735300285749,
          "user_tz": -540,
          "elapsed": 22208,
          "user": {
            "displayName": "Jason SJ Yim",
            "userId": "00695299170620821850"
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e32a2f8739db4cf38a8abce4cd8cc8b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac2a1934b54a42678bacb2c27df436b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3dcb8bb9f3a491aaa5bc21ff5722384"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4b9faf44484434ab068aef180cae6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>1 (4.65)</b></text></td><td><text style=\"padding-right:2em\"><b>The movie was not bad as mentioned by critics. It was in fact awesome; I enjoyed the whole time</b></text></td><td><text style=\"padding-right:2em\"><b>12.92</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mentioned                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> critics                    </font></mark><mark style=\"background-color: hsl(120, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fact                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> awesome                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ;                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enjoyed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> whole                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49m8hDLlWd8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}